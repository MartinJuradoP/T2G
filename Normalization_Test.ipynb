{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f5196d-8a29-4c2e-92c2-456fc2a97b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizador robusto de Mentions/Entities sobre el texto del doc ===\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "from typing import List, Dict, Tuple\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "LABEL_COLORS = {\n",
    "    \"PERSON\": \"#ffcc80\",  # naranja claro\n",
    "    \"ORG\":    \"#ff7f0e\",  # naranja\n",
    "    \"EMAIL\":  \"#9467bd\",\n",
    "    \"URL\":    \"#d62728\",\n",
    "    \"DATE\":   \"#2ca02c\",\n",
    "    \"MONEY\":  \"#1f77b4\",\n",
    "    \"TITLE\":  \"#bcbd22\",\n",
    "    \"DEGREE\": \"#17becf\",\n",
    "    \"LOC\":    \"#1abc9c\",\n",
    "    \"PRODUCT\":\"#8c564b\",\n",
    "    \"ID\":     \"#7f7f7f\",\n",
    "    \"OTHER\":  \"#aaaaaa\",\n",
    "}\n",
    "\n",
    "def _read_json(p: Path) -> dict:\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def load_sentences(doc_id: str, sentences_dir=\"outputs_sentences\") -> dict:\n",
    "    p = Path(sentences_dir) / f\"{doc_id}_sentences.json\"\n",
    "    if not p.exists(): raise FileNotFoundError(p)\n",
    "    return _read_json(p)\n",
    "\n",
    "def load_mentions(doc_id: str, mentions_dir=\"outputs_mentions\") -> dict:\n",
    "    p = Path(mentions_dir) / f\"{doc_id}_mentions.json\"\n",
    "    if not p.exists(): raise FileNotFoundError(p)\n",
    "    return _read_json(p)\n",
    "\n",
    "def load_entities(doc_id: str, entities_dir=\"outputs_entities\") -> dict:\n",
    "    p = Path(entities_dir) / f\"{doc_id}_entities.json\"\n",
    "    if not p.exists(): raise FileNotFoundError(p)\n",
    "    return _read_json(p)\n",
    "\n",
    "def build_text_and_offsets(sentences_doc: dict) -> Tuple[str, List[int], List[str]]:\n",
    "    sents = [s.get(\"text\",\"\") for s in sentences_doc.get(\"sentences\",[])]\n",
    "    offsets = []\n",
    "    parts = []\n",
    "    cur = 0\n",
    "    for i, s in enumerate(sents):\n",
    "        offsets.append(cur)\n",
    "        parts.append(s)\n",
    "        cur += len(s)\n",
    "        if i < len(sents) - 1:\n",
    "            parts.append(\"\\n\")\n",
    "            cur += 1\n",
    "    return \"\".join(parts), offsets, sents\n",
    "\n",
    "# ---------- Normalización suave para búsqueda ----------\n",
    "def _norm(s: str) -> str:\n",
    "    s = s.replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s, flags=re.M)\n",
    "    return s.strip()\n",
    "\n",
    "def _build_norm_map(original: str) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Devuelve (texto_normalizado, map_idx) donde map_idx[i_norm] = i_original.\n",
    "    Simplifica: colapsa runs de whitespace pero mantiene un mapeo claro.\n",
    "    \"\"\"\n",
    "    norm_chars = []\n",
    "    idx_map = []\n",
    "    i = 0\n",
    "    L = len(original)\n",
    "    while i < L:\n",
    "        ch = original[i]\n",
    "        if ch.isspace():\n",
    "            # colapsar espacios consecutivos a 1 espacio en la normalización\n",
    "            norm_chars.append(\" \")\n",
    "            idx_map.append(i)\n",
    "            while i < L and original[i].isspace():\n",
    "                i += 1\n",
    "            continue\n",
    "        else:\n",
    "            norm_chars.append(ch)\n",
    "            idx_map.append(i)\n",
    "            i += 1\n",
    "    normed = \"\".join(norm_chars)\n",
    "    # Recorta extremos si inicia/termina con espacio\n",
    "    if normed and normed[0] == \" \":\n",
    "        normed = normed[1:]\n",
    "        idx_map = idx_map[1:]\n",
    "    if normed and normed[-1] == \" \":\n",
    "        normed = normed[:-1]\n",
    "        idx_map = idx_map[:-1]\n",
    "    return normed, idx_map\n",
    "\n",
    "def _search_span_in_text(surface: str, doc_text: str) -> Tuple[int,int] | None:\n",
    "    \"\"\"Búsqueda tolerante: normaliza ambos y usa regex con límites suaves.\"\"\"\n",
    "    if not surface: return None\n",
    "    norm_doc, idx_map = _build_norm_map(doc_text)\n",
    "    norm_surface = _norm(surface)\n",
    "    if not norm_surface: return None\n",
    "    # \\b no siempre sirve con diacríticos; usamos patrón laxo con espacios colapsados\n",
    "    pat = re.escape(norm_surface)\n",
    "    m = re.search(pat, norm_doc, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        # prueba con boundaries laxos: ignora puntuación adyacente\n",
    "        pat = r\"(?<!\\w)\" + re.escape(norm_surface) + r\"(?!\\w)\"\n",
    "        m = re.search(pat, norm_doc, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    s_norm, e_norm = m.start(), m.end()\n",
    "    # mapear a índices originales\n",
    "    if s_norm >= len(idx_map) or e_norm-1 >= len(idx_map):\n",
    "        return None\n",
    "    s_orig = idx_map[s_norm]\n",
    "    e_orig = idx_map[e_norm-1] + 1\n",
    "    return (s_orig, e_orig)\n",
    "\n",
    "# ---------- Construcción de spans para Mentions ----------\n",
    "def spans_from_mentions(mentions_doc: dict, sentences_doc: dict, doc_text: str) -> List[Dict]:\n",
    "    _, offsets, _ = build_text_and_offsets(sentences_doc)\n",
    "    spans = []\n",
    "\n",
    "    for m in mentions_doc.get(\"entities\", []):\n",
    "        lbl = (m.get(\"canonical_label\") or m.get(\"label\") or \"OTHER\").upper()\n",
    "        txt = (m.get(\"text\") or \"\").strip()\n",
    "        si  = m.get(\"sentence_idx\")\n",
    "        span = m.get(\"char_span\")\n",
    "\n",
    "        if isinstance(si, int) and isinstance(span, (list,tuple)) and len(span)==2:\n",
    "            s, e = int(span[0]), int(span[1])\n",
    "            if 0 <= si < len(offsets):\n",
    "                s, e = offsets[si] + s, offsets[si] + e\n",
    "                spans.append({\"start\": s, \"end\": e, \"label\": lbl, \"text\": txt})\n",
    "                continue\n",
    "\n",
    "        # fallback: busca en todo el doc\n",
    "        hit = _search_span_in_text(txt, doc_text)\n",
    "        if hit:\n",
    "            spans.append({\"start\": hit[0], \"end\": hit[1], \"label\": lbl, \"text\": txt})\n",
    "\n",
    "    # quitar solapes (simple: greedy por longitud)\n",
    "    spans.sort(key=lambda x: (x[\"start\"], -(x[\"end\"]-x[\"start\"])))\n",
    "    cleaned, last_end = [], -1\n",
    "    for s in spans:\n",
    "        if s[\"start\"] >= last_end:\n",
    "            cleaned.append(s); last_end = s[\"end\"]\n",
    "    return cleaned\n",
    "\n",
    "# ---------- Construcción de spans para Entities (normalizadas) ----------\n",
    "def spans_from_entities(entities_doc: dict, mentions_doc: dict, sentences_doc: dict, doc_text: str) -> List[Dict]:\n",
    "    # index por mention_id\n",
    "    by_id = { str(m.get(\"id\")): m for m in mentions_doc.get(\"entities\", []) }\n",
    "    _, offsets, _ = build_text_and_offsets(sentences_doc)\n",
    "\n",
    "    spans = []\n",
    "    for ent in entities_doc.get(\"entities\", []):\n",
    "        lbl = (ent.get(\"type\") or \"OTHER\").upper()\n",
    "        mlist = ent.get(\"mentions\") or []\n",
    "        anchor_span = None\n",
    "        anchor_text = None\n",
    "        # 1) intentar con la primera mención que tenga indices\n",
    "        for mid in mlist:\n",
    "            md = by_id.get(str(mid))\n",
    "            if not md: continue\n",
    "            anchor_text = (md.get(\"text\") or \"\").strip()\n",
    "            si = md.get(\"sentence_idx\")\n",
    "            span = md.get(\"char_span\")\n",
    "            if isinstance(si, int) and isinstance(span, (list,tuple)) and len(span)==2 and 0 <= si < len(offsets):\n",
    "                s, e = offsets[si] + int(span[0]), offsets[si] + int(span[1])\n",
    "                anchor_span = (s, e)\n",
    "                break\n",
    "        # 2) fallback: buscar superficie de la entidad (name/value/raw) en doc\n",
    "        if not anchor_span:\n",
    "            candidates = [\n",
    "                (ent.get(\"name\") or \"\").strip(),\n",
    "                (ent.get(\"value\") or \"\").strip(),\n",
    "            ]\n",
    "            # prueba attrs obvias\n",
    "            a = ent.get(\"attrs\") or {}\n",
    "            for k in (\"org_core\",\"org_key\",\"email_norm\",\"url_norm\",\"given_name\",\"family_name\"):\n",
    "                v = (a.get(k) or \"\").strip()\n",
    "                if v and v not in candidates: candidates.append(v)\n",
    "            for surf in candidates:\n",
    "                hit = _search_span_in_text(surf, doc_text)\n",
    "                if hit:\n",
    "                    anchor_text = surf\n",
    "                    anchor_span = hit\n",
    "                    break\n",
    "        if anchor_span:\n",
    "            spans.append({\"start\": anchor_span[0], \"end\": anchor_span[1], \"label\": lbl, \"text\": anchor_text})\n",
    "\n",
    "    # quitar solapes\n",
    "    spans.sort(key=lambda x: (x[\"start\"], -(x[\"end\"]-x[\"start\"])))\n",
    "    cleaned, last_end = [], -1\n",
    "    for s in spans:\n",
    "        if s[\"start\"] >= last_end:\n",
    "            cleaned.append(s); last_end = s[\"end\"]\n",
    "    return cleaned\n",
    "\n",
    "# ---------- Render ----------\n",
    "def render_spans(doc_text: str, spans: list[dict], title=None):\n",
    "    try:\n",
    "        from spacy import displacy\n",
    "        payload = {\n",
    "            \"text\": doc_text,\n",
    "            \"ents\": [{\"start\": s[\"start\"], \"end\": s[\"end\"], \"label\": s[\"label\"]} for s in spans],\n",
    "            \"title\": title,\n",
    "        }\n",
    "        html = displacy.render(payload, style=\"ent\", options={\"colors\": LABEL_COLORS}, manual=True, jupyter=False)\n",
    "        display(HTML(html))\n",
    "    except Exception:\n",
    "        # Fallback simple: inserta <mark> (sin solapamientos)\n",
    "        html = doc_text\n",
    "        for e in sorted(spans, key=lambda x: x[\"start\"], reverse=True):\n",
    "            start, end, label = e[\"start\"], e[\"end\"], e[\"label\"]\n",
    "            color = LABEL_COLORS.get(label, \"#ffff00\")\n",
    "            snippet = (\n",
    "                f\"<mark style=\\\"background:{color}; padding:0 2px; border-radius:3px\\\">\"\n",
    "                f\"{html[start:end]} <small>({label})</small></mark>\"\n",
    "            )\n",
    "            html = html[:start] + snippet + html[end:]\n",
    "        display(HTML(f\"<div style='font-family:monospace; white-space:pre-wrap'>{html}</div>\"))\n",
    "\n",
    "\n",
    "def visualize_mentions(doc_id: str,\n",
    "                       sentences_dir=\"outputs_sentences\",\n",
    "                       mentions_dir=\"outputs_mentions\",\n",
    "                       title=None):\n",
    "    sdoc = load_sentences(doc_id, sentences_dir)\n",
    "    doc_text, _, _ = build_text_and_offsets(sdoc)\n",
    "    mdoc = load_mentions(doc_id, mentions_dir)\n",
    "    spans = spans_from_mentions(mdoc, sdoc, doc_text)\n",
    "    render_spans(doc_text, spans, title or f\"{doc_id} · Mentions\")\n",
    "\n",
    "def visualize_entities(doc_id: str,\n",
    "                       sentences_dir=\"outputs_sentences\",\n",
    "                       mentions_dir=\"outputs_mentions\",\n",
    "                       entities_dir=\"outputs_entities\",\n",
    "                       title=None):\n",
    "    sdoc = load_sentences(doc_id, sentences_dir)\n",
    "    doc_text, _, _ = build_text_and_offsets(sdoc)\n",
    "    mdoc = load_mentions(doc_id, mentions_dir)\n",
    "    edoc = load_entities(doc_id, entities_dir)\n",
    "    spans = spans_from_entities(edoc, mdoc, sdoc, doc_text)\n",
    "    render_spans(doc_text, spans, title or f\"{doc_id} · Entities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9cabf7-4e76-4ca6-8e59-fb7ca0ce46a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style=\"margin: 0\">DOC-9587970CF687 · Mentions</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Table With Collapsed Borders Job Title Email address \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Anna Fitzgerald\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " example company.com \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Smith\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " Marketing \n",
       "<mark class=\"entity\" style=\"background: #bcbd22; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manager\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TITLE</span>\n",
       "</mark>\n",
       "|lexample2Ocompany.com \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zendaya Grace\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " CEO.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2 style=\"margin: 0\">DOC-9587970CF687 · Entities</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Table With Collapsed Borders Job Title Email address \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Anna Fitzgerald\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " example company.com \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Smith\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " Marketing \n",
       "<mark class=\"entity\" style=\"background: #bcbd22; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manager\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TITLE</span>\n",
       "</mark>\n",
       "|lexample2Ocompany.com \n",
       "<mark class=\"entity\" style=\"background: #ffcc80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zendaya Grace\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " CEO.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reemplaza con tu DOC-* real (sin sufijo)\n",
    "doc_id = \"DOC-9587970CF687\"\n",
    "\n",
    "# 1) Ver menciones crudas (NER/RE) pintadas en el texto completo\n",
    "visualize_mentions(doc_id)    # como en tu screenshot\n",
    "visualize_entities(doc_id)    # entidades normalizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfba960-03c9-4602-9f0d-dd0a27530007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merge_savings': {'input_mentions': 4,\n",
       "  'entities': 4,\n",
       "  'saved': 0,\n",
       "  'saved_rate': 0.0},\n",
       " 'type_distribution': {'PERSON': 3, 'TITLE': 1},\n",
       " 'date_precision': {},\n",
       " 'money_currency': {},\n",
       " 'email_domains_top5': [],\n",
       " 'url_domains_top5': [],\n",
       " 'person_single_token_suspects': [],\n",
       " 'person_org_conflicts': 0,\n",
       " 'key_uniqueness': {'PERSON': 1.0, 'TITLE': 1.0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === JUPYTER CELL 2A: métricas usando el módulo normalizer.metrics ===\n",
    "from pathlib import Path\n",
    "import json\n",
    "from normalizer.metrics import summary as entities_summary\n",
    "\n",
    "p = Path(\"outputs_entities\") / f\"{doc_id}_entities.json\"\n",
    "de = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "s = entities_summary(de)\n",
    "s  # se imprime un dict con KPIs útiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2dc7c7-d107-42b1-a90c-862f322787dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merge_savings': {'input_mentions': 4,\n",
       "  'entities': 4,\n",
       "  'saved': 0,\n",
       "  'saved_rate': 0.0},\n",
       " 'type_distribution': {'PERSON': 3, 'TITLE': 1},\n",
       " 'date_precision': {},\n",
       " 'money_currency': {}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === JUPYTER CELL 2B: resumen inline (sin normalizer.metrics) ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def quick_entities_summary(de_doc: dict):\n",
    "    ents = de_doc.get(\"entities\", [])\n",
    "    counters = de_doc.get(\"meta\", {}).get(\"counters\", {})\n",
    "    input_mentions = int(counters.get(\"input_mentions\", 0))\n",
    "    entities = int(counters.get(\"entities\", len(ents)))\n",
    "    saved = max(0, input_mentions - entities)\n",
    "    saved_rate = (saved / input_mentions) if input_mentions else 0.0\n",
    "\n",
    "    type_dist = Counter([e.get(\"type\",\"OTHER\") for e in ents])\n",
    "\n",
    "    date_prec = Counter()\n",
    "    for e in ents:\n",
    "        if e.get(\"type\") == \"DATE\":\n",
    "            date_prec[str(e.get(\"attrs\", {}).get(\"precision\",\"unknown\")).lower()] += 1\n",
    "\n",
    "    money_stats = defaultdict(list)\n",
    "    for e in ents:\n",
    "        if e.get(\"type\") == \"MONEY\":\n",
    "            a = e.get(\"attrs\", {}) or {}\n",
    "            cur = str(a.get(\"currency\",\"UNK\"))\n",
    "            val = a.get(\"normalized_value\", None)\n",
    "            if isinstance(val, (int,float)):\n",
    "                money_stats[cur].append(float(val))\n",
    "    money_out = {}\n",
    "    for cur, vals in money_stats.items():\n",
    "        if vals:\n",
    "            money_out[cur] = {\"count\": len(vals), \"min\": min(vals), \"max\": max(vals), \"avg\": sum(vals)/len(vals)}\n",
    "        else:\n",
    "            money_out[cur] = {\"count\": 0, \"min\": None, \"max\": None, \"avg\": None}\n",
    "\n",
    "    return {\n",
    "        \"merge_savings\": {\"input_mentions\": input_mentions, \"entities\": entities, \"saved\": saved, \"saved_rate\": round(saved_rate,4)},\n",
    "        \"type_distribution\": dict(type_dist),\n",
    "        \"date_precision\": dict(date_prec),\n",
    "        \"money_currency\": money_out,\n",
    "    }\n",
    "\n",
    "p = Path(\"outputs_entities\") / f\"{doc_id}_entities.json\"\n",
    "de = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "quick_entities_summary(de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb34f-8e91-48a4-8eed-e07e2bca2f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
