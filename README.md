¬°total! aqu√≠ tienes tu **README** con **la misma secuencia** que compartiste, agregando **la etapa de Triples** (CLI, YAML, contratos y m√©tricas), y actualizando estructura/salidas.

---

# üìö Proyecto T2G ‚Äî Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular* para convertir documentos heterog√©neos (PDF, DOCX, im√°genes) en una **Representaci√≥n Intermedia (IR) homog√©nea**, segmentarlos en **chunks** sem√°nticos, luego en **oraciones filtradas**, y finalmente extraer **triples (S,R,O)** listos para RAG/IE/grafos.

* **Entrada:** PDF / DOCX / IMG
* **Salidas (hoy):** **IR (JSON)** ‚Üí **Chunks (JSON)** ‚Üí **Sentences (JSON)** ‚Üí **Triples (JSON)**
* **Dise√±o:** subsistemas desacoplados, contratos claros, ejecuci√≥n CLI/YAML

---

## ‚ú® Objetivos

* Convertir documentos heterog√©neos en una **IR homog√©nea JSON** con bloques y tablas.
* Desarrollar, probar y orquestar los **subsistemas**: Parser, Chunker, Sentence/Filter, Triples (dep.), (NER/RE), Normalizaci√≥n, Publicaci√≥n, Retriever, Evaluaci√≥n.
* Sentar base para **grafos de conocimiento**, **QA empresarial** y **compliance**.
* Mantener una arquitectura **escalable y modular**, con **contratos Pydantic** y CLIs consistentes.

---

## üß© Subsistemas (vivos y planeados)

| N¬∫ | Subsistema          | Rol                                                      | I/O                                     | Estado |
| -: | ------------------- | -------------------------------------------------------- | --------------------------------------- | ------ |
|  1 | **Parser**          | Unificar formatos a **IR JSON/MD** con layout y tablas   | Doc ‚Üí **IR**                            | ‚úÖ      |
|  2 | **HybridChunker**   | Chunks **cohesivos** con tama√±os estables y solapamiento | IR ‚Üí **Chunks**                         | ‚úÖ      |
|  3 | **Sentence/Filter** | Dividir en **oraciones** y filtrar ruido antes de IE     | Chunks ‚Üí **Sentences**                  | ‚úÖ      |
|  4 | **Triples (dep.)**  | (S,R,O) ligeros ES/EN (spaCy + regex)                    | Sentences ‚Üí **Triples**                 | ‚úÖ      |
|  5 | Extracci√≥n (NER/RE) | Entidades y relaciones                                   | Sentences ‚Üí Menciones                   | üïí     |
|  6 | Normalizaci√≥n       | Fechas, montos, IDs, orgs                                | Menciones ‚Üí Entidades                   | üïí     |
|  7 | Publicaci√≥n         | √çndices / grafo 1-hop                                    | Chunks/Ent/Triples ‚Üí ES/Qdrant/PG/Grafo | üïí     |
|  8 | Retriever (cascada) | Recall ‚Üí Precisi√≥n                                       | Query ‚Üí Contexto                        | üïí     |
|  9 | Evaluaci√≥n & HITL   | Calidad / drift / lazo humano                            | Respuestas ‚Üí Scores                     | üïí     |

---

## üìÇ Estructura del proyecto

```
project_T2G/
‚îú‚îÄ‚îÄ docs/                      # Documentos de prueba (PDF, DOCX, PNG, JPG)
‚îú‚îÄ‚îÄ parser/                    # Subsistema 1: Parser
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ chunker/                   # Subsistema 2: HybridChunker
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_chunker.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ sentence_filter/           # Subsistema 3: Sentence/Filter
‚îÇ   ‚îú‚îÄ‚îÄ sentence_filter.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ triples/                   # Subsistema 4: Triples (dep.)
‚îÇ   ‚îú‚îÄ‚îÄ dep_triples.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.yaml          # Pipeline declarativo (YAML)
‚îú‚îÄ‚îÄ outputs_ir/                # Salidas IR (JSON)
‚îú‚îÄ‚îÄ outputs_chunks/            # Salidas Chunks (JSON)
‚îú‚îÄ‚îÄ outputs_sentences/         # Salidas Sentences (JSON)
‚îú‚îÄ‚îÄ outputs_triples/           # Salidas Triples (JSON)
‚îú‚îÄ‚îÄ outputs_metrics/           # Reportes/CSVs/plots generados en notebooks
‚îú‚îÄ‚îÄ t2g_cli.py                 # CLI unificado (parse, chunk, sentences, triples, pipeline-yaml)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> **Nota:** aseg√∫rate de que `docs/` exista; ah√≠ van tus archivos de prueba. Ejemplos t√≠picos:
>
> ```
> docs/
> ‚îú‚îÄ‚îÄ Resume Martin Jurado_CDAO_24.pdf
> ‚îú‚îÄ‚îÄ tabla.png
> ‚îî‚îÄ‚îÄ ejemplo.docx
> ```

---

## üß† Qu√© hace cada etapa

### Parser (Doc ‚Üí IR)

Convierte PDF/DOCX/IMG a una **IR homog√©nea** con bloques de texto y tablas:

* **Detecci√≥n de tipo** por MIME/extensi√≥n y env√≠o a parser especializado.
* **PDF (pdfplumber):** texto por l√≠neas, tablas b√°sicas (`vertical/horizontal_strategy=lines`) y **fallback OCR** (Tesseract) por p√°gina si no hay texto/tabla.
* **DOCX (python-docx):** p√°rrafos, *headings* por estilo, tablas por celda; devuelve una **p√°gina l√≥gica**.
* **IMG (Pillow + Tesseract):** OCR directo con normalizaci√≥n.
* **Normalizaci√≥n:** `normalize_whitespace`, `dehyphenate`.
* **Metadatos:** `size_bytes`, `page_count`, `mime`, `source_path`, `created_at`.

**Contrato (IR):** `DocumentIR.pages[*].blocks` puede contener **dicts** o **modelos Pydantic** (`TextBlock`, `TableBlock`, `FigureBlock`), con campos de layout/OCR/provenance.

---

### HybridChunker (IR ‚Üí Chunks)

Genera **chunks sem√°nticos** ‚â§ `max_chars`, respetando l√≠mites naturales y a√±adiendo **solapamiento**:

1. **Aplanado** IR a `{kind, text, page_idx, block_idx}`:

   * `heading` ‚Üí prefijos `#` para conservar jerarqu√≠a (opci√≥n para **pegar** con siguiente bloque).
   * `paragraph` ‚Üí texto limpio.
   * `table` ‚Üí **serializaci√≥n CSV-like** por filas a texto plano.
2. **Empaquetado codicioso**: acumula hasta `target_chars`, corta por oraci√≥n con *spaCy* o **regex** si excede.
3. **Solapamiento** (`overlap_chars`) para continuidad.
4. **Clasificaci√≥n**: `table`, `text`, `mixed`.

**Flags clave:** `target_chars`, `max_chars`, `min_chars`, `overlap`, `sentence_splitter` (`auto|spacy|regex`), `table_policy` (`isolate|merge`).

---

### Sentence/Filter (Chunks ‚Üí Sentences)

Divide **chunks** en **oraciones** y filtra ruido:

1. **Normaliza** (espacios, guiones partidos, bullets).
2. **Divide en oraciones** con *spaCy* (si est√°) o **regex** (robusta).
3. **Filtra**: `min_chars`, `drop_stopword_only`, `drop_numeric_only`, `dedupe` (`none|exact|fuzzy`) con `fuzzy_threshold`.
4. **Trazabilidad**: `chunk_id`, `page_span`, offsets en chunk, filtros aplicados.

---

### Triples (Sentences ‚Üí Triples)

Extrae **triples (Sujeto, Relaci√≥n, Objeto)** ligeros con enfoque biling√ºe **ES/EN**:

* **Reglas de dependencias (spaCy, opcional):**

  * `VERB_dobj_nsubj` (SVO directo), `VERB_prep_pobj_nsubj` (relaci√≥n compuesta: *works\_at*, *in*‚Ä¶),
  * `nsubj_cop_attr` (copulares: *X es Y* / *X is Y*),
  * `NOUN_prep_pobj` (nominales con preposici√≥n),
  * `NOUN_appos_NOUN` ‚Üí `alias`,
  * `PASSIVE_agent_pobj` (voz pasiva de adquisici√≥n: *Y fue adquirido por X* / *Y was acquired by X*),
  * `VERB_work_prep_org` (empleo/cargo: *X trabaja en Y* / *X works at Y*).
* **Fallback regex** (si spaCy no est√° o no detecta):

  * copulares (*es/is*), adquisici√≥n (*acquired/compra/adquiri√≥*),
  * empleo/cargo (*is {title} at/in*, *trabaja en*),
  * preposicionales gen√©ricas (*of/in/with/‚Ä¶*),
  * pasiva de adquisici√≥n (*fue adquirido por / was acquired by*).
* **Anti-ruido:** ignora l√≠neas *contact-like* (URLs, emails, tel√©fonos, CP, `#` headings, l√≠neas dominadas por n√∫meros/puntuaci√≥n).
* **Canonicalizaci√≥n opcional** de relaciones (ES/EN ‚Üí forma can√≥nica, p.ej., `trabaja_en`/`works_at` ‚Üí `works_at`; `adquiri√≥`/`acquired` ‚Üí `acquired`). Desact√≠vala con `--no-canonicalize-relations`.
* **Confianza (`conf`)** por regla (regex gen√©ricas suelen ser 0.60; dependencias m√°s altas). Filtra con `--min-conf-keep` (recomendado: **0.66**).

**Salida:** `DocumentTriples` con lista de `TripleIR` (S,R,O) + `meta` por triple (regla, `conf`, `lang`, `sentence_idx`, `char_span`, `rel_surface`) y contadores globales.

---

## ‚öôÔ∏è Instalaci√≥n

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# OCR (si planeas usar fallback OCR en PDF y/o IMG)
# macOS:  brew install tesseract tesseract-lang
# Ubuntu: sudo apt install tesseract-ocr tesseract-ocr-spa

# (Opcional) spaCy para mejores cortes y dependencias (Triples)
pip install spacy
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

---

## üöÄ Uso r√°pido (CLI)

### Parse ‚Üí IR

```bash
python t2g_cli.py parse docs/Resume\ Martin\ Jurado_CDAO_24.pdf --outdir outputs_ir
```

### IR ‚Üí Chunks

```bash
python t2g_cli.py chunk outputs_ir/DOC-XXXX.json --outdir outputs_chunks \
  --sentence-splitter spacy --target-chars 1400 --max-chars 2048 --overlap 120
```

### Chunks ‚Üí Sentences

```bash
python t2g_cli.py sentences outputs_chunks/DOC-XXXX_chunks.json \
  --outdir outputs_sentences \
  --sentence-splitter auto --min-chars 25 --dedupe fuzzy --fuzzy-threshold 0.92
```

### Sentences ‚Üí Triples

```bash
python t2g_cli.py triples outputs_sentences/DOC-XXXX_sentences.json \
  --outdir outputs_triples \
  --lang auto --ruleset default-bilingual \
  --spacy auto --max-triples-per-sentence 4 \
  --enable-ner \
  --min-conf-keep 0.66
# tip: a√±ade --no-canonicalize-relations si quieres conservar la superficie cruda
```

---

## üßæ Pipeline declarativo (YAML)

Archivo por defecto: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: chunk
    args:
      clean_outdir: true
      ir_glob: "outputs_ir/*.json"
      outdir: "outputs_chunks"
      target_chars: 1400
      max_chars: 2048
      min_chars: 400
      overlap: 120
      table_policy: "isolate"    # isolate | merge
      sentence_splitter: "auto"  # auto | spacy | regex

  - name: sentences
    args:
      clean_outdir: true
      chunks_glob: "outputs_chunks/*_chunks.json"
      outdir: "outputs_sentences"
      sentence_splitter: "auto"  # o "spacy"
      min_chars: 25
      dedupe: "fuzzy"            # none | exact | fuzzy
      fuzzy_threshold: 0.92
      # toggles
      no_normalize_whitespace: false
      no_dehyphenate: false
      no_strip_bullets: false
      keep_stopword_only: false
      keep_numeric_only: false

  - name: triples
    args:
      clean_outdir: true
      sent_glob: "outputs_sentences/*_sentences.json"
      outdir: "outputs_triples"
      lang: "auto"
      ruleset: "default-bilingual"
      spacy: "auto"
      max_triples_per_sentence: 4
      keep_pronouns: false
      enable_ner: false
      canonicalize_relations: true
      min_conf_keep: 0.66
```

Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
# o expl√≠cito:
python t2g_cli.py pipeline-yaml --file pipelines/pipeline.yaml
```

---

## üìä M√©tricas por subsistema

### Parser

* **`percent_docs_ok`**: % de documentos parseados sin error (√©xito de lote).
* **`layout_loss`**: proporci√≥n `unknown/total` (proxy de p√©rdida de estructura).
* **`table_consistency`**: tablas encontradas vs. valor *golden* (si existe).

**Umbrales sugeridos:** `%docs_ok ‚â• 95%`, `layout_loss ‚â§ 0.15`, `table_consistency.ratio ‚â• 0.9`.

```python
import json, glob
from parser.schemas import DocumentIR
from parser.metrics import percent_docs_ok, layout_loss, table_consistency

irs = []
for path in sorted(glob.glob("outputs_ir/*.json")):
    try:
        irs.append(DocumentIR(**json.load(open(path, "r", encoding="utf-8"))))
    except Exception:
        continue

ok_pct = percent_docs_ok(irs)
losses = {ir.doc_id: layout_loss(ir) for ir in irs}
cons   = {ir.doc_id: table_consistency(ir) for ir in irs}

print({"percent_docs_ok": round(ok_pct, 2)})
print({"layout_loss_avg": round(sum(losses.values())/max(1,len(losses)), 4)})
print({"table_consistency_sample": list(cons.items())[:2]})
```

---

### HybridChunker

* **`chunk_length_stats`**: distribuci√≥n de longitudes.
* **`percent_within_threshold(min,max)`**: proporci√≥n de chunks dentro del rango (p. ej. 400‚Äì2048).
* **`table_mix_ratio`**: proporci√≥n `text/mixed/table`.

**Umbrales sugeridos:** `within(400‚Äì2048) ‚â• 0.95`.

```python
import json, glob
from chunker.schemas import DocumentChunks
from chunker.metrics import chunk_length_stats, percent_within_threshold, table_mix_ratio

for path in sorted(glob.glob("outputs_chunks/*_chunks.json"))[:3]:
    dc = DocumentChunks(**json.load(open(path, "r", encoding="utf-8")))
    print(dc.doc_id, {
        "len_stats": chunk_length_stats(dc),
        "within_400_2048": percent_within_threshold(dc, 400, 2048),
        "mix": table_mix_ratio(dc)
    })
```

---

### Sentence/Filter

* **`meta.counters`** en salida: `total_split`, `kept`, `dropped_short`, `dropped_stopword`, `dropped_numeric`, `dropped_dupe`, `keep_rate`.
* **`sentence_length_stats`**: distribuci√≥n de longitudes de oraciones.
* **`unique_ratio`**: proporci√≥n de oraciones √∫nicas (detecta duplicados).

**Umbrales sugeridos:** `keep_rate` saludable ‚âà 0.6‚Äì0.9 seg√∫n dominio; `unique_ratio ‚â• 0.85`.

```python
import json, glob
from sentence_filter.schemas import DocumentSentences
from sentence_filter.metrics import sentence_length_stats, unique_ratio

for path in sorted(glob.glob("outputs_sentences/*_sentences.json"))[:3]:
    ds = DocumentSentences(**json.load(open(path, "r", encoding="utf-8")))
    counters = ds.meta.get("counters", {})
    print(ds.doc_id, {
        "keep_rate": round(counters.get("keep_rate", 0), 3),
        "kept": counters.get("kept"),
        "len_stats": sentence_length_stats(ds),
        "unique_ratio": round(unique_ratio(ds), 3)
    })
```

---

### Triples

* **Agregados:** n¬∫ de documentos, **n¬∫ de triples**, **unique ratio** (`drop_duplicates` por S,R,O).
* **Distribuci√≥n de relaciones** y **reglas** (qu√© patrones producen qu√©).
* **`conf` stats** y efecto de `--min-conf-keep`.
* **Auditor√≠a con contexto:** join contra oraciones por `sentence_idx`.

```python
import json, glob, pandas as pd
from triples.schemas import DocumentTriples

rows = []
for p in sorted(glob.glob("outputs_triples/*_triples.json")):
    dt = DocumentTriples(**json.load(open(p)))
    for t in dt.triples:
        rows.append({
          "doc_id": dt.doc_id,
          "subject": t.subject, "relation": t.relation, "object": t.object,
          "dep_rule": t.meta.get("dep_rule"), "conf": t.meta.get("conf"),
          "lang": t.meta.get("lang"), "sentence_idx": t.meta.get("sentence_idx"),
          "file": p
        })

df = pd.DataFrame(rows)
print("Docs:", df["doc_id"].nunique(), "| Triples:", len(df))
print("Unique ratio:", df.drop_duplicates(["subject","relation","object"]).shape[0]/max(1,len(df)))
print("Top relaciones:\n", df["relation"].value_counts().head())
print("Top reglas:\n", df["dep_rule"].value_counts().head())
print("Conf stats:\n", df["conf"].describe())
```

**Auditor√≠a con oraci√≥n original:**

```python
import json, pandas as pd

doc = "DOC-XXXX"
dt = json.load(open(f"outputs_triples/{doc}_triples.json"))
ds = json.load(open(f"outputs_sentences/{doc}_sentences.json"))

trip = pd.json_normalize(dt["triples"])
trip["sentence_idx"] = trip["meta.sentence_idx"]
sents = pd.DataFrame(ds["sentences"]).assign(sentence_idx=lambda d: range(len(d)))
audit = trip.merge(sents[["sentence_idx","text"]], on="sentence_idx", how="left")

cols = ["subject","relation","object","meta.conf","meta.dep_rule","text"]
print(audit[cols].head(12))
```

**Sugerencias de calidad:**

* Usa `--min-conf-keep 0.66` para recortar `regex_prep` gen√©ricas.
* Si hay ruido de preposiciones, **sube** a 0.70‚Äì0.72 o activa spaCy (`--spacy force`) para m√°s precisi√≥n estructural.
* Desactiva `--no-canonicalize-relations` si necesitas la superficie cruda (√∫til para depuraci√≥n).

---

## üõ†Ô∏è Troubleshooting

* **JSONs vac√≠os/corruptos:** escritura **at√≥mica**, el runner **salta** JSON inv√°lidos. Limpia y re-ejecuta:

  ```bash
  rm -rf outputs_ir/* outputs_chunks/* outputs_sentences/* outputs_triples/*
  python t2g_cli.py pipeline-yaml
  ```
* **spaCy no carga / modelos faltantes:** instala `es_core_news_sm` y `en_core_web_sm` o usa `--spacy off` (regex-only).
* **OCR fallback no funciona:** instala Tesseract y verifica `PATH` (o `tesseract_cmd` en Windows).
* **Imports fallan:** ejecuta desde la **ra√≠z** (`python t2g_cli.py ‚Ä¶`).
* **Triples con ‚Äúof/in/with‚Äù dominantes:** eleva `--min-conf-keep` o fuerza spaCy.
* **Pandas `InvalidIndexError` en auditor√≠as:** usa `reset_index(drop=True)` antes de `concat/merge`.
* **Carpetas con ‚Äúruido‚Äù de corridas previas:** en YAML pon `clean_outdir: true` en cada etapa.

---

## üß™ Roadmap inmediato

* NER/RE y normalizaci√≥n (fechas, montos, IDs).
* Publicaci√≥n a ES/Qdrant/Postgres/Grafo (con esquemas para triples).
* M√©tricas de evaluaci√≥n y lazo humano (RAGAS + HITL).
* Tests (`pytest`) y *golden sets* de regresi√≥n.

---

## üìö Referencias

* **OCR:** PaddleOCR, Tesseract
* **PDF:** pdfplumber
* **DOCX:** python-docx
* **NLP:** spaCy
* **Papers:** DocRED (ACL 2019), K-Adapter (ACL 2020), AutoNER (ACL 2018), KG-BERT (AAAI 2020)

---

## ‚öñÔ∏è Licencia

MIT (o la que definas).
