# ğŸ“š Proyecto T2G â€” Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular y extensible* que convierte documentos heterogÃ©neos (PDF, DOCX, imÃ¡genes) en una **RepresentaciÃ³n Intermedia (IR) homogÃ©nea**, los enriquece con **contexto semÃ¡ntico global y local**, y prepara la base para construir **grafos de conocimiento** y sistemas de **bÃºsqueda avanzada (RAG, QA, compliance, etc.)**.

* **Entrada :** PDF / DOCX / PNG / JPG
* **Salidas :**

  * `DocumentIR (JSON)`
  * `DocumentIR+Topics (JSON)`
* **Salidas futuras:** Chunks, Mentions, Entities, Triples, NormalizaciÃ³n, Grafo.
* **DiseÃ±o:** subsistemas **desacoplados**, contratos **Pydantic**, orquestaciÃ³n vÃ­a **CLI + YAML**.

---

## âœ¨ Objetivos

* Unificar la ingesta de documentos en una **IR JSON comÃºn** independientemente del formato.
* Enriquecer documentos con **contexto semÃ¡ntico a nivel documento y chunk** usando **embeddings + BERTopic**.
* Mantener una arquitectura **resiliente, escalable y modular**: cada subsistema puede ejecutarse de forma independiente.
* Preparar la base para **grafos de conocimiento**, **QA empresarial**, **compliance regulatorio** y **sistemas RAG**.

---

## ğŸ§© Subsistemas

| NÂº | Subsistema                       | Rol principal                                                               | Entrada             | Salida                   | Estado |
| -: | -------------------------------- | --------------------------------------------------------------------------- | ------------------- | ------------------------ | ------ |
|  1 | **Parser**                       | Genera **IR JSON** homogÃ©nea con metadatos y layout                         | Doc (PDF/DOCX/IMG)  | `DocumentIR` JSON        | âœ…      |
|  2 | **BERTopic Contextizer (doc)**   | Asigna **tÃ³picos y keywords globales** a nivel documento                    | `DocumentIR`        | `DocumentIR+Topics` JSON | âœ…      |
|  3 | **HybridChunker**                | Segmenta documento en **chunks semÃ¡nticos estables (â‰¤2048 tokens)**         | `DocumentIR+Topics` | `DocumentChunks` JSON    | âœ…     |
|  4 | **BERTopic Contextizer (chunk)** | Asigna tÃ³picos locales a cada chunk (subtemas); enlaza con tÃ³picos globales | `DocumentChunks`    | `Chunks+Topics` JSON     | âœ…     |
|  5 | **Adaptive Schema Selector**     | Define dinÃ¡micamente entidades relevantes segÃºn contexto                    | `Chunks+Topics`     | `SchemaSelection` JSON   | ğŸ”œ     |
|  6 | **Mentions (NER/RE)**            | Detecta menciones condicionadas por tÃ³picos                                 | `Chunks+Topics`     | `Mentions` JSON          | ğŸ”œ     |
|  7 | **Clustering de Menciones**      | Agrupa spans en clusters semÃ¡nticos                                         | `Mentions` JSON     | `Clusters` JSON          | ğŸ”œ     |
|  8 | **Weak Supervision / Label**     | Etiqueta clusters de alta confianza (Snorkel-style)                         | `Clusters`          | `LabeledClusters` JSON   | ğŸ”œ     |
|  9 | **LLM Intervention**             | Clasifica clusters ambiguos con **few-shot prompting o prototipos**         | `Clusters`          | `RefinedLabels` JSON     | ğŸ”œ     |
| 10 | **NormalizaciÃ³n (hÃ­brida)**      | Canonicaliza entidades y estandariza representaciones                       | `Mentions/Clusters` | `Entities` JSON          | ğŸ”œ     |
| 11 | **Graph Export**                 | Publica entidades y relaciones en grafos (Neo4j, GraphDB, RDF/SHACL)        | `Entities+Triples`  | Grafo / DB               | ğŸ”œ     |

---

## ğŸ“‚ Estructura del proyecto

```bash
project_T2G/
â”œâ”€â”€ docs/                            # Documentos de prueba y ejemplos
â”‚   â”œâ”€â”€ samples/
â”‚   â””â”€â”€ benchmarks/
â”‚
â”œâ”€â”€ parser/                          # Subsistema Parser
â”‚   â”œâ”€â”€ parsers.py                   # LÃ³gica de parseo (PDF, DOCX, IMG)
â”‚   â”œâ”€â”€ metrics.py                   # MÃ©tricas: %docs_ok, layout_loss, table_consistency
â”‚   â”œâ”€â”€ schemas.py                   # Contratos Pydantic (DocumentIR)
â”‚   â”œâ”€â”€ helpers.py                   # Utilidades comunes de parsing
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ contextizer/                     # Subsistema Contextizer hÃ­brido
â”‚   â”œâ”€â”€ contextizer.py               # Orquestador principal (doc- y chunk-level)
â”‚   â”œâ”€â”€ metrics.py                   # MÃ©tricas bÃ¡sicas de cobertura, redundancia, etc.
â”‚   â”œâ”€â”€ metrics_ext.py               # MÃ©tricas extendidas (coherence_semantic, entropy, etc.)
â”‚   â”œâ”€â”€ analyzers.py                 # Router adaptativo (avg_len, TTR, semantic_var)
â”‚   â”œâ”€â”€ models.py                    # Clases internas (TopicItem, ContextizerResult)
â”‚   â”œâ”€â”€ schemas.py                   # Contratos Pydantic (DocumentTopics, ChunkTopics)
â”‚   â”œâ”€â”€ utils.py                     # NormalizaciÃ³n, stopwords, embeddings, caching
â”‚   â”œâ”€â”€ hybrid/                      # NÃºcleo del modo hÃ­brido
â”‚   â”‚   â”œâ”€â”€ hybrid_contextizer.py    # FusiÃ³n TF-IDF + KeyBERT + embeddings + DBSCAN
â”‚   â”‚   â”œâ”€â”€ density_clustering.py    # Clustering semÃ¡ntico adaptativo
â”‚   â”‚   â”œâ”€â”€ keyword_fusion.py        # FusiÃ³n hÃ­brida de scores
â”‚   â”‚   â”œâ”€â”€ mmr.py                   # Maximal Marginal Relevance (diversificaciÃ³n)
â”‚   â”‚   â”œâ”€â”€ analyzers.py             # Analizadores estadÃ­sticos por bloque
â”‚   â”‚   â”œâ”€â”€ metrics_ext.py           # MÃ©tricas de topic-coherence y redundancia
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ schema_selector/                 # Adaptive Schema Selector
â”‚   â”œâ”€â”€ registry.py                  # OntologÃ­as y dominios (medical, legal, etc.)
â”‚   â”œâ”€â”€ schemas.py                   # Contratos Pydantic
â”‚   â”œâ”€â”€ selector.py                  # LÃ³gica de scoring y selecciÃ³n adaptativa
â”‚   â”œâ”€â”€ utils.py                     # Funciones auxiliares (similitud, normalizaciÃ³n)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ pipelines/                       # ConfiguraciÃ³n declarativa del pipeline
â”‚   â””â”€â”€ pipeline.yaml
â”‚
â”œâ”€â”€ outputs_ir/                      # Salidas intermedias (IR JSON)
â”œâ”€â”€ outputs_doc_topics/              # DocumentIR + Topics (doc-level)
â”œâ”€â”€ outputs_chunks/                  # Chunks enriquecidos (chunk-level)
â”œâ”€â”€ outputs_schema/                  # SelecciÃ³n de esquemas adaptativos
â”‚
â”œâ”€â”€ t2g_cli.py                       # CLI unificado para orquestaciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## ğŸ§  Etapas explicadas

---

### 1) Parser (Doc â†’ IR) âœ…

**Entrada:** PDF / DOCX / PNG / JPG
**Salida:** `DocumentIR` (`outputs_ir/{DOC}_*.json`)

**QuÃ© hace:**

* Detecta formato y selecciona parser:

  * **PDF:** texto + tablas (pdfplumber); OCR fallback si escaneado.
  * **DOCX:** pÃ¡rrafos, headings, tablas (python-docx).
  * **IMG:** OCR (pytesseract).
* Normaliza espacios, guiones cortados, saltos de lÃ­nea.
* AÃ±ade metadatos: `sha256`, `mime`, `page_count`, `size_bytes`.

**Ejemplo de salida:**

```json
{
  "doc_id": "DOC-12345",
  "pages": [
    {
      "page_number": 1,
      "blocks": [
        {"type": "paragraph", "text": "Los leones viven en Ãfrica..."},
        {"type": "table", "cells": [{"row":0,"col":0,"text":"Dato"}]}
      ]
    }
  ],
  "meta": {
    "mime":"application/pdf",
    "page_count":1,
    "sha256":"â€¦",
    "source_path":"docs/leones.pdf"
  }
}
```

---

### 2) Hybrid Contextizer (doc-level) âœ…

**Entrada:** `DocumentIR` (`outputs_ir/*.json`)
**Salida:** `DocumentIR+Topics` (`outputs_doc_topics/*.json`)

---

**QuÃ© hace (paso a paso):**

1. **ExtracciÃ³n y normalizaciÃ³n del texto**

   * Toma todos los bloques textuales (`pages[].blocks[].text`) del IR.
   * Aplica limpieza ligera:

     * Colapsa espacios en blanco.
     * Elimina caracteres no textuales (`â€¢`, `Â¶`, `â€”`, etc.).
     * Sustituye comillas y apÃ³strofes para uniformidad.
   * Cada bloque se conserva con su trazabilidad:

     * `page_number`, `block_index`, `type`.
   * Si el texto estÃ¡ vacÃ­o o contiene menos de 3 caracteres, se descarta.

   **Resultado:** un conjunto de bloques vÃ¡lidos `texts` de tamaÃ±o `n`, normalizados y listos para anÃ¡lisis semÃ¡ntico.

---

2. **CÃ¡lculo de embeddings globales**

   Se usa el modelo configurado (`SentenceTransformer` con `cfg.embedding_model`).

   ```math
   E_i = f_{ST}(b_i)
   ```

   donde cada $E_i$ es un vector en $\mathbb{R}^d$.

   Luego se calcula un vector promedio para representar el contexto general del documento:

   ```math
   \bar{E}_{doc} = \frac{1}{n}\sum_{i=1}^{n} E_i
   ```

   Este embedding global sirve para medir coherencia temÃ¡tica y variaciÃ³n semÃ¡ntica entre bloques.

---

3. **Router adaptativo (heurÃ­sticas del modo hÃ­brido)**

   El mÃ³dulo `analyzers.py` determina si debe ejecutarse el **modo hÃ­brido**.
   EvalÃºa tres mÃ©tricas simples pero efectivas:

   | MÃ©trica                             | FÃ³rmula          | Umbral   | Significado            |        |                       |
   | ----------------------------------- | ---------------- | -------- | ---------------------- | ------ | --------------------- |
   | Longitud media (`avg_len`)          | $\frac{1}{n}\sum | b_i      | $                      | `< 45` | texto corto o ruidoso |
   | Diversidad lÃ©xica (`TTR`)           | $\frac{V}{T}$    | `> 0.5`  | mucha variaciÃ³n lÃ©xica |        |                       |
   | Varianza semÃ¡ntica (`semantic_var`) | $Var(E_i)$       | `> 0.25` | temas dispersos        |        |                       |

   El hÃ­brido se activa si **2 o mÃ¡s** condiciones son verdaderas:

   ```json
   "reason": "doc-hybrid"
   ```

   Este paso evita usar mÃ©todos costosos de clustering o reducciÃ³n de dimensiÃ³n en textos pequeÃ±os o de baja densidad.

---

4. **Clustering semÃ¡ntico por densidad**

   Se aplica **DBSCAN** directamente sobre los embeddings para detectar grupos semÃ¡nticos sin predefinir `n_topics`:

   ```math
   cluster(E_i) = 
   \begin{cases}
   k, & \text{si } \text{dist}_\text{cosine}(E_i, E_j) < \varepsilon \\
   -1, & \text{ruido}
   \end{cases}
   ```

   ParÃ¡metros:

   ```math
   \varepsilon = 0.25, \quad \text{min\_samples}=2
   ```

   La ventaja de DBSCAN es que **no requiere conocer cuÃ¡ntos temas existen**; se adapta a la estructura semÃ¡ntica del documento.

---

5. **ConstrucciÃ³n de tÃ³picos y keywords**

   Una vez formados los clusters, el mÃ³dulo `density_clustering.py` construye tÃ³picos equivalentes a `TopicItem`:

   ```math
   topics = \{ t_k = (\text{keywords}, \text{exemplar}, \text{count}) \}
   ```

   Cada tÃ³pico $t_k$ se resume con:

   * **Exemplar:** bloque mÃ¡s representativo (mÃ¡xima similitud media dentro del cluster).
   * **Count:** nÃºmero de bloques asignados.
   * **Keywords:** extraÃ­das mediante la fusiÃ³n hÃ­brida de seÃ±ales lÃ©xicas y semÃ¡nticas.

---

6. **ExtracciÃ³n y fusiÃ³n de keywords (TF-IDF + KeyBERT + Embeddings)**

   Se combina informaciÃ³n de tres fuentes:

   1. **Relevancia lÃ©xica (TF-IDF):**

      ```math
      S_{tfidf}(w) = tf(w) \cdot \log\frac{N}{df(w)}
      ```

      EvalÃºa la importancia del tÃ©rmino dentro del documento.

   2. **Relevancia contextual (KeyBERT, opcional):**

      ```math
      S_{keybert}(w) = \cos(\vec{w}, \bar{E}_{doc})
      ```

      Mide alineaciÃ³n semÃ¡ntica con el contexto global.

   3. **CohesiÃ³n semÃ¡ntica (embeddings):**

      ```math
      S_{emb}(w) = \frac{1}{k}\sum_{i=1}^{k}\cos(\vec{E_i}, \vec{w})
      ```

   Las tres se fusionan ponderadamente:

   ```math
   S_{hybrid}(w) = 0.5 S_{tfidf}(w) + 0.3 S_{keybert}(w) + 0.2 S_{emb}(w)
   ```

   Esta ponderaciÃ³n surge de experimentos que equilibran precisiÃ³n contextual y estabilidad en documentos pequeÃ±os.

---

7. **SelecciÃ³n de keywords por Maximal Marginal Relevance (MMR)**

   Se aplica el filtro MMR (`mmr.py`) para eliminar sinÃ³nimos y redundancia:

   ```math
   MMR(w_i) = \lambda \cos(\vec{w_i}, \vec{t}) - (1-\lambda)\max_{w_j\in S}\cos(\vec{w_i}, \vec{w_j})
   ```

   con $\lambda = 0.7$.

   Resultado: un conjunto reducido de keywords informativas y no redundantes.

---

8. **CÃ¡lculo de mÃ©tricas extendidas**

   Las mÃ©tricas cuantitativas (`metrics_ext.py`) permiten auditar la calidad semÃ¡ntica:

   | MÃ©trica                  | DescripciÃ³n             | FÃ³rmula                               |                |    |   |    |
   | ------------------------ | ----------------------- | ------------------------------------- | -------------- | -- | - | -- |
   | `entropy_topics`         | DispersiÃ³n de tÃ³picos   | $- \sum p_j \log p_j$                 |                |    |   |    |
   | `redundancy_score`       | Redundancia media       | $1 - \frac{V_\text{Ãºnico}}{V}$        |                |    |   |    |
   | `keywords_diversity_ext` | Diversidad global       | $\frac{                               | V_\text{Ãºnico} | }{ | V | }$ |
   | `semantic_variance`      | Varianza de embeddings  | $Var(E_{exemplar})$                   |                |    |   |    |
   | `coherence_semantic`     | Coherencia intra-tÃ³pico | $\overline{\cos(E_{kw_i}, E_{kw_j})}$ |                |    |   |    |

---

9. **Salida (JSON)**

```json
"topics_doc": {
  "reason": "doc-hybrid",
  "n_samples": 12,
  "n_topics": 3,
  "keywords_global": ["mercado","acciones","inversiÃ³n"],
  "topics": [
    {"topic_id":0,"count":4,"exemplar":"El mercado bursÃ¡til sube tras reporte trimestral...","keywords":["acciones","finanzas","subida"]},
    {"topic_id":1,"count":5,"exemplar":"Informe de inflaciÃ³n mensual afecta inversiÃ³n...","keywords":["inflaciÃ³n","inversiÃ³n","monetaria"]},
    {"topic_id":2,"count":3,"exemplar":"Perspectivas globales para 2025...","keywords":["economÃ­a","riesgo","global"]}
  ],
  "metrics": {
    "redundancy_score": 0.19,
    "semantic_variance": 0.38,
    "entropy_topics": 0.72,
    "coherence_semantic": 0.88
  }
}
```

---

10. **Beneficios tÃ©cnicos del enfoque hÃ­brido**

| DimensiÃ³n         | Mejora                                                   |
| ----------------- | -------------------------------------------------------- |
| Robustez          | Maneja textos breves y ruidosos sin colapsar.            |
| Interpretabilidad | Cada tÃ³pico conserva su contexto original.               |
| Estabilidad       | No requiere hiperparÃ¡metros ajustados.                   |
| Trazabilidad      | Cada decisiÃ³n se justifica con `meta.reason` y mÃ©tricas. |
| Escalabilidad     | Reutiliza embeddings y evita pasos costosos.             |

---



### 3) HybridChunker âœ…

**Entrada:** `DocumentIR+Topics` (`outputs_doc_topics/*.json`)

**Salida:** `DocumentChunks` (`outputs_chunks/*.json`)

**QuÃ© hace (paso a paso):**

1. **ExtracciÃ³n de bloques base**

   * Toma todos los bloques textuales del `DocumentIR`.
   * Cada bloque conserva trazabilidad (`page_number`, `block_indices`) para poder mapear chunks a posiciones exactas en el documento.
   * Filtra bloques vacÃ­os o no textuales (imÃ¡genes, tablas sin OCR).

2. **SegmentaciÃ³n semÃ¡ntica hÃ­brida**
   Se combinan varias estrategias para dividir el documento en **chunks coherentes de â‰¤2048 tokens** (Ã³ptimo para LLMs):

   * **Reglas de headings:** patrones tÃ­picos (`IntroducciÃ³n`, `MÃ©todos`, `Conclusiones`, etc.) detectados vÃ­a regex.
     Esto evita cortar secciones temÃ¡ticas de forma arbitraria.

   * **spaCy sentence boundaries:** segmenta pÃ¡rrafos largos en oraciones completas, preservando la coherencia sintÃ¡ctica.
     Si spaCy no estÃ¡ disponible, se aplica un fallback mediante puntuaciÃ³n (`.`, `;`, `?`, `!`) o saltos de lÃ­nea dobles (`\n\n`).

   * **Empaquetado semÃ¡ntico por longitud:** agrupa oraciones hasta alcanzar un lÃ­mite aproximado de tokens.
     Controla umbrales de tamaÃ±o (`min_chars`, `max_chars`, `max_tokens`) para mantener chunks **equilibrados** en densidad y contexto.
     â†’ El resultado son **unidades estables**: suficientemente largas para el contexto, pero sin sobrepasar los lÃ­mites Ã³ptimos de procesamiento.

3. **Herencia de contexto (`topic_hints`)**

   * Cada chunk hereda informaciÃ³n del doc-level contextizer (`topic_ids`, `keywords_global`, `topic_affinity`).
   * Esto asegura **consistencia semÃ¡ntica vertical**: lo que el documento conoce a nivel global se transfiere a los fragmentos locales.
   * La afinidad entre el chunk y los tÃ³picos globales se calcula mediante una mezcla semÃ¡ntico-lÃ©xica:

     $$\text{topic affinity blend}(c, t) = \alpha \cdot \cos(\vec{c}, \vec{t}) + (1 - \alpha) \cdot J(c, t)$$

     Donde:

     * $\vec{c}$ y $\vec{t}$ son los embeddings del chunk y del topic.
     * $J(c, t)$ es la similitud lÃ©xica (*Jaccard*).
     * $\alpha$ controla el peso semÃ¡ntico (por defecto, $\alpha = 0.7$).

     Esta combinaciÃ³n hace al sistema mÃ¡s robusto frente a textos cortos (tweets, clÃ¡usulas legales o notas clÃ­nicas) donde la semÃ¡ntica sola puede ser insuficiente.

4. **CÃ¡lculo de mÃ©tricas locales**
   
   Para evaluar la calidad y la coherencia de los chunks, se calculan mÃ©tricas cuantitativas:

   * `cohesion_vs_doc`: mide la similitud coseno entre el embedding del chunk y el embedding promedio del documento.
     Representa **quÃ© tan bien el fragmento conserva el contexto global**.

     $$\text{cohesion vs doc}(c_i) = \cos(\vec{c_i}, \bar{\vec{D}})$$

   * `max_redundancy`: mide la similitud mÃ¡xima del chunk con cualquier otro chunk dentro del mismo documento.
     Detecta **fragmentos repetitivos o duplicados**.

     $$\text{max redundancy}(c_i) = \max_{j \neq i} \cos(\vec{c_i}, \vec{c_j})$$

   * `redundancy_norm`: versiÃ³n normalizada de la redundancia, que ajusta el valor segÃºn el tamaÃ±o relativo del fragmento.

     $$\text{redundancy norm}(c_i) = \text{max redundancy}(c_i) \times \frac{\text{len}(c_i)}{\text{avg len(chunks)}}$$

     Esto penaliza mÃ¡s a los chunks **largos y redundantes**, y reduce el impacto de los fragmentos **cortos pero similares**.
     En textos como contratos, reseÃ±as o tweets, mejora la detecciÃ³n de contenido **repetitivo** versus **informativo**.

   * `novelty`: mide la proporciÃ³n de informaciÃ³n nueva que aporta cada fragmento.
     Es complementaria a la redundancia.

     $$\text{novelty}(c_i) = 1 - \text{max redundancy}(c_i)$$

     Un valor alto de `novelty` indica que el chunk aporta **contexto Ãºnico o evidencia nueva**.

   * `chunk_health`: mÃ©trica compuesta que pondera la cohesiÃ³n y la novedad penalizando la redundancia.
     Resume la **salud semÃ¡ntica del fragmento**.

     $$\text{chunk health}(c_i) = \text{cohesion vs doc}(c_i) \times (1 - \text{max redundancy}(c_i))$$

     Este score puede usarse en etapas posteriores (por ejemplo, el **Adaptive Schema Selector**) para **ponderar o filtrar chunks** segÃºn su calidad semÃ¡ntica.

5. **SerializaciÃ³n robusta**

   * Cada chunk se guarda con un `chunk_id` Ãºnico, metadatos (`doc_id`, idioma, embeddings opcionales) y trazabilidad (`source_spans`).
   * El formato JSON conserva compatibilidad con las etapas siguientes (`contextize-chunks`, `schema-select`).
   * Si los embeddings o spaCy no estÃ¡n disponibles, aplica **fallbacks automÃ¡ticos** para mantener la robustez del pipeline.

* **Ejemplo de salida (simplificado):**

```json
{
  "chunk_id": "DOC-123_0001",
  "text": "Complicaciones Asociadas al tratamiento prolongado...",
  "topic_hints": {
    "inherited_keywords": ["diabetes", "tratamiento"],
    "inherited_topic_ids": [0, 1],
    "topic_affinity_blend": {"0": 0.82, "1": 0.71}
  },
  "scores": {
    "cohesion_vs_doc": 0.82,
    "max_redundancy": 0.59,
    "redundancy_norm": 0.73,
    "novelty": 0.41,
    "chunk_health": 0.34
  },
  "meta_local": {
    "embedding_model": "all-MiniLM-L6-v2",
    "lang": "es"
  }
}
```


---

### 4) Hybrid Contextizer (chunk-level) âœ…

**Entrada:** `DocumentChunks` (`outputs_chunks/*.json`)
**Salida:** `Chunks+Topics` (`outputs_chunks/*.json`)

---

**QuÃ© hace (paso a paso):**

1. **Carga de chunks y herencia vertical**

   * Toma los chunks creados por el `HybridChunker`.
   * Cada chunk incluye texto, contexto heredado (`topic_hints`, `keywords_global`) y mÃ©tricas locales (`cohesion_vs_doc`, `chunk_health`).
   * Se asegura **consistencia semÃ¡ntica vertical**:

     ```math
     T_{chunk} \subseteq T_{doc}
     ```

     Esto garantiza que los subtemas locales siempre estÃ©n dentro de los temas globales del documento.

---

2. **Recalculo de embeddings locales**

   Cada chunk $c_i$ se representa con un vector `SentenceTransformer`:

   ```math
   E_{c_i} = f_{ST}(c_i)
   ```

   Y se calcula un embedding global del documento:

   ```math
   \bar{E}_{doc} = \frac{1}{N}\sum_i E_{c_i}
   ```

   Este vector sirve para medir la alineaciÃ³n temÃ¡tica entre cada fragmento y el contexto general.

---

3. **Router adaptativo (modo de operaciÃ³n)**

   | CondiciÃ³n       | Modo           | AcciÃ³n                        |
   | --------------- | -------------- | ----------------------------- |
   | `n_samples = 0` | skip           | omite                         |
   | `<5`            | fallback-small | usa TF-IDF simple             |
   | `5 â‰¤ n â‰¤ 50`    | hybrid         | usa pipeline hÃ­brido completo |
   | `>50`           | hybrid-large   | usa DBSCAN mÃ¡s estricto       |

   ```json
   "reason": "chunk-hybrid"
   ```

---

4. **GeneraciÃ³n de tÃ³picos locales**

   * **TF-IDF fallback:**

     ```math
     S_{freq}(w) = \frac{f(w)}{\sum f(w)}
     ```
   * **Modo hÃ­brido completo:**

     ```math
     S_{hybrid}(w) = 0.5 S_{tfidf}(w) + 0.3 S_{keybert}(w) + 0.2 S_{emb}(w)
     ```
   * **Clustering local:**

     ```math
     \varepsilon = 0.25, \quad \text{min\_samples}=2
     ```

   Cada cluster produce un conjunto de keywords diversificado con MMR local.

---

5. **Afinidad entre tÃ³picos locales y globales**

   Se mide la coherencia semÃ¡ntica entre cada chunk y los tÃ³picos globales del documento:

   ```math
   affinity(c_i, t_j) = 0.7 \cos(E_{c_i}, E_{t_j}) + 0.3 J(c_i, t_j)
   ```

   donde $J$ es similitud lÃ©xica (*Jaccard*).

---

6. **MÃ©tricas intra-chunk**

   | MÃ©trica            | DescripciÃ³n                  | FÃ³rmula                                   |
   | ------------------ | ---------------------------- | ----------------------------------------- |
   | `local_cohesion`   | coherencia con el cluster    | $\cos(E_{c_i}, \bar{E}_{cluster})$        |
   | `local_redundancy` | similitud con chunks vecinos | $\max_{j\neq i}\cos(E_{c_i},E_{c_j})$     |
   | `novelty`          | informaciÃ³n nueva            | $1 - local_redundancy$                    |
   | `chunk_health`     | balance de calidad           | `local_cohesion Ã— (1 - local_redundancy)` |

   Estas mÃ©tricas permiten identificar fragmentos repetitivos o irrelevantes.

---

7. **Salida (JSON)**

```json
"topics_chunks": {
  "reason": "chunk-hybrid",
  "n_samples": 10,
  "n_topics": 2,
  "keywords_global": ["diabetes","tratamiento","insulina"],
  "topics": [
    {"topic_id":0,"count":6,"exemplar":"Tratamiento prolongado con insulina...","keywords":["diabetes","tratamiento","efectos","insulina"]},
    {"topic_id":1,"count":4,"exemplar":"Los sÃ­ntomas clÃ­nicos incluyen...","keywords":["sÃ­ntomas","fatiga","sed","clÃ­nico"]}
  ],
  "metrics": {
    "coverage": 0.94,
    "fallback_rate": 0.06,
    "topic_coherence_local": 0.85,
    "keywords_overlap": 0.70
  }
}
```

---

8. **Beneficios tÃ©cnicos (chunk-level)**

| DimensiÃ³n             | Mejora                                              |
| --------------------- | --------------------------------------------------- |
| Granularidad          | Detecta subtemas dentro de secciones extensas.      |
| Consistencia vertical | Mantiene alineaciÃ³n semÃ¡ntica con tÃ³picos globales. |
| Resiliencia           | Funciona incluso con pocos chunks o texto corto.    |
| MÃ©tricas internas     | EvalÃºa coherencia, redundancia y novedad.           |
| Escalabilidad         | Procesa grandes volÃºmenes en paralelo.              |
| ReutilizaciÃ³n         | Aprovecha embeddings previos del doc-level.         |

---

### 5) Adaptive Schema Selector âœ…

**Entrada:** `Chunks+Topics` (`outputs_chunks/*.json`)
**Salida:** `SchemaSelection` (`outputs_schema/*.json`)

---

**QuÃ© hace:**

El **Adaptive Schema Selector (ASS)** determina dinÃ¡micamente quÃ© **dominios de entidades** (por ejemplo, mÃ©dico, legal, financiero o genÃ©rico) son relevantes para cada documento y chunk.
Su propÃ³sito es **filtrar, priorizar y contextualizar** los tipos de entidades que deben extraerse en las etapas siguientes, mejorando la **precisiÃ³n semÃ¡ntica** del grafo y reduciendo ruido.

---

1. **Registro de dominios (`registry.py`)**

   Cada dominio estÃ¡ definido en la ontologÃ­a base (`registry.py`) y contiene:

   * **Entidades (`EntityTypeDef`)** con atributos (por ejemplo: `Disease`, `Treatment`, `Contract`, `Transaction`).
   * **Relaciones (`RelationTypeDef`)** entre entidades (por ejemplo: `treated_with`, `paid_by`, `binds`).
   * **Aliases** y vocabulario especÃ­fico en espaÃ±ol e inglÃ©s (por ejemplo: `"enfermedad"`, `"patologÃ­a"`, `"disease"` para `Disease`).
   * **Descripciones semÃ¡nticas** utilizadas para generar embeddings de referencia.

   Los dominios incluidos en la versiÃ³n `v2_bilingual` son:

   | Dominio            | Ejemplo de entidades                                   | Contextos tÃ­picos                         |
   | ------------------ | ------------------------------------------------------ | ----------------------------------------- |
   | `medical`          | `Disease`, `Symptom`, `Drug`, `Treatment`, `LabTest`   | artÃ­culos clÃ­nicos, diagnÃ³sticos          |
   | `legal`            | `Contract`, `Party`, `Obligation`, `Penalty`           | contratos, clÃ¡usulas, litigios            |
   | `financial`        | `Invoice`, `Transaction`, `StockIndicator`, `Policy`   | facturas, informes financieros            |
   | `reviews_and_news` | `Review`, `NewsArticle`, `MarketEvent`                 | reseÃ±as, noticias econÃ³micas              |
   | `ecommerce`        | `Order`, `Product`, `Review`                           | comercio electrÃ³nico, reseÃ±as de clientes |
   | `identity`         | `Person`, `Address`, `IDDocument`                      | registros, formularios                    |
   | `generic`          | `Person`, `Organization`, `Date`, `Location`, `Amount` | fallback universal                        |

---

2. **ExtracciÃ³n de seÃ±ales del documento/chunk**

   El selector combina **tres tipos de seÃ±ales** para estimar la afinidad de cada texto con los dominios registrados:

   * **Keywords**
     Se detectan coincidencias entre los tokens normalizados del documento y los `aliases` del dominio.
     Las coincidencias se ponderan por frecuencia y relevancia POS (sustantivos, nombres propios, etc.).

     $$
     S_{kw}(d) = \frac{\text{overlaps}(d)}{\text{total aliases}(d)} \times \log(1 + f_{term})
     $$

     Donde:

     * $\text{overlaps}(d)$ â†’ nÃºmero de alias del dominio encontrados.

     * $f_{term}$ â†’ frecuencia media de los tÃ©rminos coincidentes.

     > Ejemplo: un documento con â€œcontratoâ€, â€œfirmaâ€, â€œclÃ¡usulaâ€ activarÃ¡ el dominio `legal` con alto $S_{kw}$.

   * **Embeddings**
     Calcula la similitud coseno entre los **embeddings promedio del texto** y los **embeddings representativos del dominio** (precalculados a partir de sus descripciones y aliases).

     $$
     S_{emb}(d) = \cos(\vec{v}*{text}, \vec{v}*{domain})
     $$

     * $\vec{v}_{text}$ â†’ embedding medio del chunk o documento.

     * $\vec{v}_{domain}$ â†’ embedding medio del dominio.

     > Ejemplo: â€œantihipertensivoâ€ activa el dominio `medical` aunque la palabra â€œenfermedadâ€ no aparezca explÃ­citamente.

   * **Priors**
     Cada dominio tiene un peso base $P(d)$ que refleja su probabilidad a priori de aparecer.

     $$
     P(d) = \text{prior}(d) \in [0, 1]
     $$

     > Ejemplo: `generic = 0.1`, `medical = 0.05`, `legal = 0.05`
     > El dominio `generic` siempre se considera como fallback.

---

3. **FÃ³rmula de scoring (por dominio)**

   Para cada dominio $d$, se calcula un score ponderado combinando las tres seÃ±ales:

   $$
   \text{score}(d) = \alpha \cdot S_{kw}(d) + \beta \cdot S_{emb}(d) + \gamma \cdot P(d)
   $$

   Donde:

   * $S_{kw}(d)$: score normalizado por coincidencia lÃ©xica.
   * $S_{emb}(d)$: similitud coseno entre embeddings.
   * $P(d)$: prior asignado al dominio.
   * $\alpha, \beta, \gamma$: hiperparÃ¡metros configurables en `SelectorConfig`.

   **Ejemplo default:**

   $\alpha = 0.6, ; \beta = 0.3, ; \gamma = 0.1$

   â†’ mÃ¡s peso a keywords, menor a embeddings y priors.

   > En textos tÃ©cnicos (contratos, facturas) domina $\alpha$.
   > En textos conceptuales (reseÃ±as o informes), $\beta$ captura mejor la afinidad semÃ¡ntica.

---

4. **SelecciÃ³n final**

   Una vez calculados los scores, se aplica la fase de decisiÃ³n:

   * Se **ordenan** los dominios de mayor a menor score.
   * Se **descartan** los dominios con score < `min_topic_conf`.
   * Se **seleccionan** los `top-k` dominios configurados (por defecto `topk_domains = 2`).
   * Se marca `ambiguous=True` si la diferencia entre el primer y segundo dominio es menor al margen definido:

     $$
     ambiguous = |S(d_1) - S(d_2)| < \tau
     $$

     donde $\tau$ es `ambiguity_threshold` (por defecto 0.1).
   * Siempre se incluye el dominio **genÃ©rico** como fallback (`allow_fallback_generic=True`).

---

**Ejemplo de salida (simplificado):**

```json
{
  "doc": {
    "doc_id": "DOC-CA28DAF58CC7",
    "top_domains": ["financial", "reviews_and_news"],
    "ambiguous": false,
    "domain_scores": [
      {
        "domain": "financial",
        "score": 0.27,
        "evidence": [
          {"kind": "keyword", "detail": {"match": ["pago","banco","transacciÃ³n"], "kw_score": 0.23}},
          {"kind": "embedding", "detail": {"cosine": 0.84}},
          {"kind": "prior", "detail": {"value": 0.05}}
        ]
      },
      {
        "domain": "reviews_and_news",
        "score": 0.15,
        "evidence": [
          {"kind": "keyword", "detail": {"match": ["noticia","acciones"], "kw_score": 0.11}},
          {"kind": "embedding", "detail": {"cosine": 0.72}}
        ]
      }
    ]
  },
  "chunks": [
    {
      "chunk_id": "DOC-CA28DAF58CC7_0001",
      "top_domain": "financial",
      "ambiguous": false,
      "domain_scores": [
        {"domain": "financial", "score": 0.31},
        {"domain": "reviews_and_news", "score": 0.12}
      ]
    }
  ],
  "meta": {
    "alpha": 0.6,
    "beta": 0.3,
    "gamma": 0.1,
    "always_include": ["generic"],
    "ambiguity_threshold": 0.1,
    "registry_version": "v2_bilingual"
  }
}
```

---

**Notas adicionales:**

* La afinidad entre dominios puede ajustarse con un refuerzo contextual:

  $$
  S_{\text{adj}}(d, c_i)
  = S_{\text{domain}}(d, c_i)\,\times\,
  \Big(1 + \lambda \cdot \text{cohesion\_vs\_doc}(c_i)\Big)
  $$

  donde \\( \lambda = 0.2 \\) pondera la cohesiÃ³n semÃ¡ntica entre chunk y documento.

* La inferencia final combina contexto global y local:

  $$
  S_{\text{final}}(d)
  = \omega \cdot S_{\text{doc}}(d)
  + (1 - \omega) \cdot \frac{1}{N}\sum_{i=1}^{N} S_{\text{chunk}}(d, c_i)
  $$

  con \\( \omega = 0.5 \\) por defecto.



---

**Beneficios:**

* **PrecisiÃ³n contextual:** sÃ³lo se procesan entidades coherentes con el dominio dominante.
* **Escalabilidad:** nuevos dominios pueden aÃ±adirse fÃ¡cilmente al `registry.py`.
* **Interpretabilidad:** cada decisiÃ³n conserva su `evidence_trace` (palabras clave, similitudes, priors).
* **Auditable:** todos los pesos, fÃ³rmulas y umbrales se guardan en el `meta` del JSON.
* **Consistencia vertical:** mantiene coherencia entre documento y chunks.

---

### 6) Mentions (NER/RE) ğŸ”œ

* Detecta menciones de entidades **condicionadas por tÃ³picos**.

### 7â€“11) Clustering â†’ Label â†’ LLM â†’ NormalizaciÃ³n â†’ Grafo ğŸ”œ

* Agrupan spans, etiquetan con weak supervision, refinan con LLM y normalizan entidades.
* Export final a **grafo de conocimiento**.

---


# ğŸ“‹ Flujo de informaciÃ³n en T2G (Contrado de datos)


| Etapa / Subsistema                  | **Entrada (quÃ© toma)**                  | **Salida (quÃ© agrega / construye)**                                                                               |
| ----------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **1. Parser** (Doc â†’ IR)            | Documento bruto (PDF, DOCX, IMG)        | `DocumentIR`: texto por bloques, tablas, metadatos (`mime`, `pages`, `sha256`, `source_path`).                    |
| **2. Contextizer (doc-level)**      | `DocumentIR.pages.blocks.text`          | `meta.topics_doc`: tÃ³picos globales, keywords generales, ejemplares, `outlier_ratio`.                             |
| **3. HybridChunker**                | `DocumentIR+Topics`                     | `chunks[*]`: segmentos â‰¤2048 tokens. Heredan `topic_hints` + mÃ©tricas (`cohesion`, `redundancy`).                 |
| **4. Contextizer (chunk-level)**    | `chunks.text` + `topic_hints` heredados | `chunks[*].topic`: tÃ³pico local con `topic_id`, `keywords`, `prob` + `meta.topics_chunks`.                        |
| **5. Schema Selector (adaptativo)** | `chunks+topics` + `registry`            | `SchemaSelection`: dominios relevantes por doc y chunk + `evidence_trace` (keywords, embeddings, priors, scores). |
                            |

---
## ğŸ“‚ Pipeline declarativo (YAML)

Archivo: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: contextize-doc
    args:
      clean_outdir: true
      ir_glob: "outputs_ir/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      nr_topics: null
      seed: 42
      outdir: "outputs_doc_topics"

  - name: chunk
    args:
      clean_outdir: true
      ir_glob: "outputs_doc_topics/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      spacy_model: "es_core_news_sm"
      max_tokens: 2048
      min_chars: 280
      outdir: "outputs_chunks"

  - name: contextize-chunks
    args:
      clean_outdir: false
      ir_glob: "outputs_chunks/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      nr_topics: null
      seed: 42
      outdir: "outputs_chunks"
      
  - name: schema-select
    args:
      chunks_glob: "outputs_chunks/*.json"
      outdir: "outputs_schema"
      clean_outdir: true
      alpha_kw: 0.6
      beta_emb: 0.3
      gamma_prior: 0.1
      ambig_margin: 0.08
      topk_domains: 2
      topk_entity_types: 5
```

Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
```

---

## ğŸ“Š MÃ©tricas por subsistema

---

### Parser 
EvalÃºa la calidad y consistencia del parseo de documentos heterogÃ©neos.

- `percent_docs_ok`: proporciÃ³n de documentos parseados sin errores.
- `layout_loss`: pÃ©rdida de estructura visual o de formato.
- `table_consistency`: coherencia entre tablas detectadas y esperadas.
- `ocr_ratio`: porcentaje de pÃ¡ginas procesadas mediante OCR (indicador de calidad visual).
- `avg_parse_time`: tiempo promedio de procesamiento por documento.
- `block_density`: nÃºmero promedio de bloques vÃ¡lidos por pÃ¡gina.

---

### Contextizer (doc-level) 
Mide la calidad del modelado temÃ¡tico global.

- `coverage`: proporciÃ³n de bloques asignados a algÃºn tÃ³pico.
- `outlier_rate`: ratio de bloques descartados por ruido o baja densidad.
- `topic_size_stats`: distribuciÃ³n del tamaÃ±o de clusters (`min`, `median`, `p95`).
- `keywords_diversity`: diversidad de palabras clave Ãºnicas (riqueza semÃ¡ntica).
- `topic_stability`: correlaciÃ³n promedio entre embeddings de tÃ³picos en runs sucesivos (indicador de consistencia temporal).
- `topic_entropy`: medida de dispersiÃ³n temÃ¡tica (mayor = mÃ¡s heterogeneidad).

---

### HybridChunker 
EvalÃºa la **coherencia**, **redundancia** y **salud semÃ¡ntica** de los fragmentos.

#### ğŸ”¹ MÃ©tricas base
- `chunk_length_stats`: distribuciÃ³n de tamaÃ±os (caracteres / tokens).
- `cohesion_vs_doc`: similitud coseno entre embedding de chunk y embedding global del documento.  
  $$\text{cohesion\_vs\_doc}(c_i) = \cos(\vec{c_i}, \bar{\vec{D}})$$
- `max_redundancy`: similitud mÃ¡xima entre embeddings de chunks.  
  $$\text{max redundancy}(c_i) = \max_{j \neq i} \cos(\vec{c_i}, \vec{c_j})$$
- `redundancy_norm`: redundancia ajustada por longitud.  
  $$\text{redundancy norm}(c_i) = \text{max redundancy}(c_i) \times \frac{\text{len}(c_i)}{\text{avg len(chunks)}}$$
- `novelty`: proporciÃ³n de informaciÃ³n nueva aportada.  
  $$\text{novelty}(c_i) = 1 - \text{max redundancy}(c_i)$$

#### ğŸ”¹ MÃ©tricas compuestas
- `chunk_health`: salud semÃ¡ntica = cohesiÃ³n Ã— (1 âˆ’ redundancia).  
  $$\text{chunk health}(c_i) = \text{cohesion\_vs\_doc}(c_i) \times (1 - \text{max redundancy}(c_i))$$
- `semantic_density`: proporciÃ³n de tokens relevantes (sin stopwords) sobre el total.
- `lexical_density`: densidad lÃ©xica medida por tÃ©rminos significativos / totales.
- `type_token_ratio`: diversidad de vocabulario (variedad lÃ©xica).
- `semantic_coverage`: % de chunks con cohesiÃ³n â‰¥ 0.7 (bien alineados al documento).
- `redundancy_flag_rate`: % de chunks con redundancia excesiva â‰¥ 0.6.
- `topic_affinity_blend`: afinidad semÃ¡ntico-lÃ©xica con los tÃ³picos globales del documento.  
  $$\text{topic affinity blend}(c,t) = \alpha \cos(\vec{c}, \vec{t}) + (1-\alpha)J(c,t)$$

#### ğŸ”¹ MÃ©tricas globales
- `global_health_score`: indicador compuesto (`good`, `moderate`, `poor`).
- `avg_chunk_health`: promedio de salud semÃ¡ntica global.
- `coverage_ratio`: proporciÃ³n de texto total cubierto por chunks vÃ¡lidos.
- `oversegmentation_rate`: % de chunks demasiado pequeÃ±os (bajo umbral `min_chars`).
- `undersegmentation_rate`: % de chunks demasiado largos (superan `max_tokens`).

---

### Contextizer (chunk-level) 
EvalÃºa la coherencia temÃ¡tica local y su relaciÃ³n con los tÃ³picos globales.

- `coverage`: % de chunks con tÃ³pico asignado.
- `fallback_rate`: % de documentos donde se usÃ³ fallback en lugar de BERTopic.
- `topic_size_stats`: distribuciÃ³n de tamaÃ±os de subtemas.
- `keywords_overlap`: solapamiento promedio entre keywords globales y locales.
- `topic_coherence_local`: coherencia intracluster promedio (similitud coseno media entre embeddings de un mismo tema).
- `local_entropy`: dispersiÃ³n de tÃ³picos locales (mide estabilidad semÃ¡ntica).

---

### Adaptive Schema Selector 
EvalÃºa la **relevancia y precisiÃ³n contextual** del mapeo dominioâ€“documento.

#### ğŸ”¹ MÃ©tricas base
- `domain_score_distribution`: histograma de scores por dominio.  
  $$\text{score}(d) = \alpha S_{\text{kw}}(d) + \beta S_{\text{emb}}(d) + \gamma P(d)$$
- `coverage_domains`: nÃºmero promedio de dominios relevantes por documento.
- `ambiguity_rate`: % de documentos o chunks marcados como `ambiguous = True`.
- `domain_confidence_gap`: diferencia entre el primer y segundo dominio (medida de separabilidad).
- `prior_influence`: peso efectivo de los priors sobre el score final.
- `always_included_rate`: % de documentos donde el dominio genÃ©rico fue incluido por fallback.

#### ğŸ”¹ MÃ©tricas de refuerzo contextual
- `contextual_boost_effect`: variaciÃ³n media del score tras aplicar refuerzo semÃ¡ntico.  
  $$\Delta S = S_{\text{adj}} - S_{\text{domain}}$$
- `lambda_effectiveness`: sensibilidad del refuerzo de cohesiÃ³n (variaciÃ³n promedio por unidad de Î»).  
  $$\eta_\lambda = \frac{\Delta S}{\lambda}$$

#### ğŸ”¹ MÃ©tricas de calidad ontolÃ³gica
- `schema_alignment`: similitud promedio entre entidades detectadas y entidades esperadas del dominio.
- `entity_type_coverage`: % de tipos de entidad del dominio detectados al menos una vez.
- `relation_type_coverage`: % de relaciones del dominio identificadas.
- `ontology_diversity`: nÃºmero de dominios distintos presentes en el corpus.

#### ğŸ”¹ MÃ©tricas globales
- `domain_precision`: proporciÃ³n de dominios correctamente asignados (vs. gold standard si existe).
- `domain_recall`: proporciÃ³n de dominios relevantes detectados.
- `domain_f1`: media armÃ³nica entre precisiÃ³n y recall (solo si hay ground truth disponible).
- `evidence_trace`: trazabilidad completa de evidencias (keywords, embeddings, priors, scores).

---



---

## âš™ï¸ InstalaciÃ³n

### Requisitos principales

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### OCR

```bash
brew install tesseract tesseract-lang   # macOS
sudo apt install tesseract-ocr-spa      # Ubuntu/Debian
```

### NLP / embeddings

```bash
pip install spacy sentence-transformers bertopic
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

### Opcionales

```bash
pip install torch joblib matplotlib wordcloud
```

---

## ğŸš€ Uso rÃ¡pido (CLI)

```bash
# Parser â†’ IR
python t2g_cli.py parse docs/ejemplo.pdf --outdir outputs_ir

# Contextizer (doc) â†’ IR+Topics
python t2g_cli.py contextize-doc outputs_ir/*.json --outdir outputs_doc_topics

# Pipeline completo
python t2g_cli.py pipeline-yaml
```

---

## ğŸ› ï¸ Troubleshooting

* **JSONs vacÃ­os:** borra outputs y reejecuta.
* **OCR falla:** instala Tesseract y revisa `PATH`.
* **spaCy no carga:** descarga modelos `es_core_news_sm`, `en_core_web_sm`.
* **Errores de import:** ejecuta siempre desde raÃ­z del proyecto.

---

## ğŸ§ª Roadmap inmediato

* AÃ±adir **HybridChunker** con herencia de `topics_doc`.
* Implementar **chunk-level contextizer**.
* Integrar **Adaptive Schema Selector**.
* Completar **Mentions + Clustering + NormalizaciÃ³n**.
* Exportar entidades y relaciones a **Neo4j / RDF**.

