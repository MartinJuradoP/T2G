# ğŸ“š Proyecto T2G â€” Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular y extensible* que convierte documentos heterogÃ©neos (PDF, DOCX, imÃ¡genes) en una **RepresentaciÃ³n Intermedia (IR) homogÃ©nea**, los enriquece con **contexto semÃ¡ntico global y local**, y prepara la base para construir **grafos de conocimiento** y sistemas de **bÃºsqueda avanzada (RAG, QA, compliance, etc.)**.

* **Entrada (hoy):** PDF / DOCX / PNG / JPG
* **Salidas (hoy):**

  * `DocumentIR (JSON)`
  * `DocumentIR+Topics (JSON)`
* **Salidas futuras:** Chunks, Mentions, Entities, Triples, NormalizaciÃ³n, Grafo.
* **DiseÃ±o:** subsistemas **desacoplados**, contratos **Pydantic**, orquestaciÃ³n vÃ­a **CLI + YAML**.

---

## âœ¨ Objetivos

* Unificar la ingesta de documentos en una **IR JSON comÃºn** independientemente del formato.
* Enriquecer documentos con **contexto semÃ¡ntico a nivel documento y chunk** usando **embeddings + BERTopic**.
* Mantener una arquitectura **resiliente, escalable y modular**: cada subsistema puede ejecutarse de forma independiente.
* Preparar la base para **grafos de conocimiento**, **QA empresarial**, **compliance regulatorio** y **sistemas RAG**.

---

## ğŸ§© Subsistemas

| NÂº | Subsistema                       | Rol principal                                                               | Entrada             | Salida                   | Estado |
| -: | -------------------------------- | --------------------------------------------------------------------------- | ------------------- | ------------------------ | ------ |
|  1 | **Parser**                       | Genera **IR JSON** homogÃ©nea con metadatos y layout                         | Doc (PDF/DOCX/IMG)  | `DocumentIR` JSON        | âœ…      |
|  2 | **BERTopic Contextizer (doc)**   | Asigna **tÃ³picos y keywords globales** a nivel documento                    | `DocumentIR`        | `DocumentIR+Topics` JSON | âœ…      |
|  3 | **HybridChunker**                | Segmenta documento en **chunks semÃ¡nticos estables (â‰¤2048 tokens)**         | `DocumentIR+Topics` | `DocumentChunks` JSON    | ğŸ”œ     |
|  4 | **BERTopic Contextizer (chunk)** | Asigna tÃ³picos locales a cada chunk (subtemas); enlaza con tÃ³picos globales | `DocumentChunks`    | `Chunks+Topics` JSON     | ğŸ”œ     |
|  5 | **Adaptive Schema Selector**     | Define dinÃ¡micamente entidades relevantes segÃºn contexto                    | `Chunks+Topics`     | `SchemaSelection` JSON   | ğŸ”œ     |
|  6 | **Mentions (NER/RE)**            | Detecta menciones condicionadas por tÃ³picos                                 | `Chunks+Topics`     | `Mentions` JSON          | ğŸ”œ     |
|  7 | **Clustering de Menciones**      | Agrupa spans en clusters semÃ¡nticos                                         | `Mentions` JSON     | `Clusters` JSON          | ğŸ”œ     |
|  8 | **Weak Supervision / Label**     | Etiqueta clusters de alta confianza (Snorkel-style)                         | `Clusters`          | `LabeledClusters` JSON   | ğŸ”œ     |
|  9 | **LLM Intervention**             | Clasifica clusters ambiguos con **few-shot prompting o prototipos**         | `Clusters`          | `RefinedLabels` JSON     | ğŸ”œ     |
| 10 | **NormalizaciÃ³n (hÃ­brida)**      | Canonicaliza entidades y estandariza representaciones                       | `Mentions/Clusters` | `Entities` JSON          | ğŸ”œ     |
| 11 | **Graph Export**                 | Publica entidades y relaciones en grafos (Neo4j, GraphDB, RDF/SHACL)        | `Entities+Triples`  | Grafo / DB               | ğŸ”œ     |

---

## ğŸ“‚ Estructura del proyecto

```bash
project_T2G/
â”œâ”€â”€ docs/                   # Documentos de prueba
â”œâ”€â”€ parser/                 # Subsistema Parser
â”‚   â”œâ”€â”€ parsers.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ contextizer/            # Subsistema Contextizer
â”‚   â”œâ”€â”€ contextizer.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ outputs_ir/             # Salidas: DocumentIR
â”œâ”€â”€ outputs_doc_topics/     # Salidas: IR+Topics
â”œâ”€â”€ t2g_cli.py              # CLI unificado
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Etapas explicadas

---

### 1) Parser (Doc â†’ IR) âœ…

**Entrada:** PDF / DOCX / PNG / JPG
**Salida:** `DocumentIR` (`outputs_ir/{DOC}_*.json`)

**QuÃ© hace:**

* Detecta formato y selecciona parser:

  * **PDF:** texto + tablas (pdfplumber); OCR fallback si escaneado.
  * **DOCX:** pÃ¡rrafos, headings, tablas (python-docx).
  * **IMG:** OCR (pytesseract).
* Normaliza espacios, guiones cortados, saltos de lÃ­nea.
* AÃ±ade metadatos: `sha256`, `mime`, `page_count`, `size_bytes`.

**Ejemplo de salida:**

```json
{
  "doc_id": "DOC-12345",
  "pages": [
    {
      "page_number": 1,
      "blocks": [
        {"type": "paragraph", "text": "Los leones viven en Ãfrica..."},
        {"type": "table", "cells": [{"row":0,"col":0,"text":"Dato"}]}
      ]
    }
  ],
  "meta": {
    "mime":"application/pdf",
    "page_count":1,
    "sha256":"â€¦",
    "source_path":"docs/leones.pdf"
  }
}
```

---

### 2) BERTopic Contextizer (doc-level) âœ…

**Entrada:** `DocumentIR` (`outputs_ir/*.json`)
**Salida:** `DocumentIR+Topics` (`outputs_doc_topics/*.json`)

**QuÃ© hace:**

* Calcula embeddings globales con **SentenceTransformers (`all-MiniLM-L6-v2`)**.
* Descubre tÃ³picos con **BERTopic**.
* Si hay pocos chunks:

  * 0â€“1 â†’ topic Ãºnico (singleton).
  * 2 â†’ fallback basado en frecuencia de tÃ©rminos.
  * 3+ â†’ BERTopic normal.
* Enriquecimiento agregado a `meta.topics_doc`:

  * `n_topics`, `keywords_global`, `exemplar`, `outlier_ratio`.

**Ejemplo de salida:**

```json
"topics_doc": {
  "reason": "doc-level",
  "n_topics": 2,
  "keywords_global": ["diabetes","insulina","sÃ­ntomas"],
  "topics": [
    {
      "topic_id": 0,
      "count": 5,
      "exemplar": "SÃ­ntomas ClÃ­nicos mÃ¡s frecuentes...",
      "keywords": ["sÃ­ntomas","micciÃ³n","aumento"]
    },
    {
      "topic_id": 1,
      "count": 4,
      "exemplar": "Diabetes Tipo 2: Un enfoque clÃ­nico",
      "keywords": ["diabetes","tratamiento","complicaciones"]
    }
  ]
}
```

---

### 3) HybridChunker ğŸ”œ

* Divide documento en **chunks semÃ¡nticos â‰¤2048 tokens**.
* Hereda `topics_doc` para mantener coherencia.

### 4) BERTopic Contextizer (chunk-level) ğŸ”œ

* Asigna tÃ³picos especÃ­ficos a cada chunk.
* Permite detectar **subtemas** y transferir contexto.

### 5) Adaptive Schema Selector ğŸ”œ

* Define dinÃ¡micamente quÃ© entidades extraer segÃºn tÃ³picos.

### 6) Mentions (NER/RE) ğŸ”œ

* Detecta menciones de entidades **condicionadas por tÃ³picos**.

### 7â€“11) Clustering â†’ Label â†’ LLM â†’ NormalizaciÃ³n â†’ Grafo ğŸ”œ

* Agrupan spans, etiquetan con weak supervision, refinan con LLM y normalizan entidades.
* Export final a **grafo de conocimiento**.

---

## ğŸ“‚ Pipeline declarativo (YAML)

Archivo: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: contextize-doc
    args:
      clean_outdir: true
      ir_glob: "outputs_ir/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      nr_topics: null
      seed: 42
      outdir: "outputs_doc_topics"
```

Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
```

---

## ğŸ“Š MÃ©tricas por subsistema

### Parser âœ…

* `percent_docs_ok`: Ã©xito de parseo por lote.
* `layout_loss`: pÃ©rdida de estructura.
* `table_consistency`: tablas detectadas vs esperadas.

### Contextizer (doc-level) âœ…

* `coverage`: proporciÃ³n de chunks asignados a algÃºn tÃ³pico.
* `outlier_rate`: ratio de outliers vs asignaciones vÃ¡lidas.
* `topic_size_stats`: distribuciÃ³n (min, mediana, p95).
* `keywords_diversity`: diversidad de keywords Ãºnicas.

### PrÃ³ximos subsistemas ğŸ”œ

* **HybridChunker**: `chunk_length_stats`.
* **Mentions**: `precision/recall vs golden`.
* **Normalization**: % de entidades deduplicadas.

---

## âš™ï¸ InstalaciÃ³n

### Requisitos principales

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### OCR

```bash
brew install tesseract tesseract-lang   # macOS
sudo apt install tesseract-ocr-spa      # Ubuntu/Debian
```

### NLP / embeddings

```bash
pip install spacy sentence-transformers bertopic
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

### Opcionales

```bash
pip install torch joblib matplotlib wordcloud
```

---

## ğŸš€ Uso rÃ¡pido (CLI)

```bash
# Parser â†’ IR
python t2g_cli.py parse docs/ejemplo.pdf --outdir outputs_ir

# Contextizer (doc) â†’ IR+Topics
python t2g_cli.py contextize-doc outputs_ir/*.json --outdir outputs_doc_topics

# Pipeline completo
python t2g_cli.py pipeline-yaml
```

---

## ğŸ› ï¸ Troubleshooting

* **JSONs vacÃ­os:** borra outputs y reejecuta.
* **OCR falla:** instala Tesseract y revisa `PATH`.
* **spaCy no carga:** descarga modelos `es_core_news_sm`, `en_core_web_sm`.
* **Errores de import:** ejecuta siempre desde raÃ­z del proyecto.

---

## ğŸ§ª Roadmap inmediato

* AÃ±adir **HybridChunker** con herencia de `topics_doc`.
* Implementar **chunk-level contextizer**.
* Integrar **Adaptive Schema Selector**.
* Completar **Mentions + Clustering + NormalizaciÃ³n**.
* Exportar entidades y relaciones a **Neo4j / RDF**.

