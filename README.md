# üìö Proyecto T2G ‚Äî Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular* para convertir documentos heterog√©neos (PDF, DOCX, im√°genes) en una **Representaci√≥n Intermedia (IR) homog√©nea**, segmentarlos en **chunks** sem√°nticos, luego en **oraciones filtradas**, y finalmente extraer **triples (S,R,O)** listos para RAG/IE/grafos.

* **Entrada:** PDF / DOCX / IMG
* **Salidas (hoy):** **IR (JSON)** ‚Üí **Chunks (JSON)** ‚Üí **Sentences (JSON)** ‚Üí **Triples (JSON)**
* **Dise√±o:** subsistemas desacoplados, contratos claros, ejecuci√≥n CLI/YAML

---

## ‚ú® Objetivos

* Convertir documentos heterog√©neos en una **IR homog√©nea JSON** con bloques y tablas.
* Desarrollar, probar y orquestar los **subsistemas**: Parser, Chunker, Sentence/Filter, Triples (dep.), **Mentions (NER/RE)**, Normalizaci√≥n, Publicaci√≥n, Retriever, Evaluaci√≥n.
* Sentar base para **grafos de conocimiento**, **QA empresarial** y **compliance**.
* Mantener una arquitectura **escalable y modular**, con **contratos Pydantic** y CLIs consistentes.

---

## üß© Subsistemas (vivos y planeados)

| N¬∫ | Subsistema            | Rol                                                      | I/O                                     | Estado |
| -: | --------------------- | -------------------------------------------------------- | --------------------------------------- | ------ |
|  1 | **Parser**            | Unificar formatos a **IR JSON/MD** con layout y tablas   | Doc ‚Üí **IR**                            | ‚úÖ      |
|  2 | **HybridChunker**     | Chunks **cohesivos** con tama√±os estables y solapamiento | IR ‚Üí **Chunks**                         | ‚úÖ      |
|  3 | **Sentence/Filter**   | Dividir en **oraciones** y filtrar ruido antes de IE     | Chunks ‚Üí **Sentences**                  | ‚úÖ      |
|  4 | **Triples (dep.)**    | (S,R,O) ligeros ES/EN (spaCy + regex)                    | Sentences ‚Üí **Triples**                 | ‚úÖ      |
|  5 | **Mentions (NER/RE)** | Menciones de entidades/relaciones + consenso con Triples | Sentences ‚Üí **Mentions**                | ‚úÖ      |
|  6 | Normalizaci√≥n         | Fechas, montos, IDs, orgs                                | Mentions ‚Üí Entidades                    | üïí     |
|  7 | Publicaci√≥n           | √çndices / grafo 1-hop                                    | Chunks/Ent/Triples ‚Üí ES/Qdrant/PG/Grafo | üïí     |
|  8 | Retriever (cascada)   | Recall ‚Üí Precisi√≥n                                       | Query ‚Üí Contexto                        | üïí     |
|  9 | Evaluaci√≥n & HITL     | Calidad / drift / lazo humano                            | Respuestas ‚Üí Scores                     | üïí     |
| 10 | **IE (orquestador)**  | Reusa/crea Triples y ejecuta Mentions con boost          | Sentences/(Chunks) ‚Üí Triples+Mentions   | ‚úÖ      |

---

## üìÇ Estructura del proyecto

```
project_T2G/
‚îú‚îÄ‚îÄ docs/                          # Documentos de prueba (PDF, DOCX, PNG, JPG)
‚îú‚îÄ‚îÄ parser/
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ chunker/
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_chunker.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ sentence_filter/
‚îÇ   ‚îú‚îÄ‚îÄ sentence_filter.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ triples/
‚îÇ   ‚îú‚îÄ‚îÄ dep_triples.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îú‚îÄ‚îÄ mentions/                      # NER/RE (con boost desde Triples y HF opcional)
‚îÇ   ‚îú‚îÄ‚îÄ ner_re.py
‚îÇ   ‚îú‚îÄ‚îÄ hf_plugins.py              # Wrapper HF local (opcional, offline)
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ validate_ie.py             # Validaci√≥n (Mentions soportadas por Triples)
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.yaml              # Pipeline declarativo (YAML)
‚îú‚îÄ‚îÄ outputs_ir/
‚îú‚îÄ‚îÄ outputs_chunks/
‚îú‚îÄ‚îÄ outputs_sentences/
‚îú‚îÄ‚îÄ outputs_triples/
‚îú‚îÄ‚îÄ outputs_mentions/
‚îú‚îÄ‚îÄ outputs_metrics/               # Reportes/CSVs/plots (notebooks)
‚îú‚îÄ‚îÄ t2g_cli.py                     # CLI unificado: parse ¬∑ chunk ¬∑ sentences ¬∑ triples ¬∑ mentions ¬∑ ie ¬∑ pipeline-yaml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> **Nota:** coloca tus archivos de prueba en `docs/`:
>
> ```
> docs/
> ‚îú‚îÄ‚îÄ Resume Martin Jurado_CDAO_24.pdf
> ‚îú‚îÄ‚îÄ tabla.png
> ‚îî‚îÄ‚îÄ ejemplo.docx
> ```

---

## üß† Qu√© hace cada etapa


> Cada etapa toma una entrada bien definida y devuelve un JSON con **contrato Pydantic**. Abajo ver√°s: **entrada ‚Üí salida**, c√≥mo decide, campos clave y flags √∫tiles.

---

### 1) Parser (Doc ‚Üí IR)

**Entrada:** PDF / DOCX / PNG / JPG
**Salida:** `DocumentIR` (`outputs_ir/{DOC}_*.json`)

**Qu√© hace:**

* Detecta tipo por **extensi√≥n/MIME** y enruta a un parser especializado.
* **PDF (pdfplumber):**

  * Extrae **texto por l√≠neas** y **tablas** usando heur√≠sticas de l√≠neas (estrategias vertical/horizontal).
  * Si una p√°gina no tiene texto/tabla, aplica **OCR por p√°gina** (Tesseract) como *fallback*.
* **DOCX (python-docx):** lee p√°rrafos y estilos (para *headings*), serializa tablas por celdas. Se considera una ‚Äúp√°gina l√≥gica‚Äù.
* **IMG (Pillow + Tesseract):** OCR directo; normaliza espacios y guiones rotos.
* **Normaliza**: `normalize_whitespace`, `dehyphenate`.
* **Anota metadatos:** `size_bytes`, `page_count`, `mime`, `source_path`, `created_at`.

**Contrato de salida (resumen):**

```json
{
  "doc_id": "DOC-XXXX",
  "pages": [
    {
      "page_idx": 0,
      "blocks": [
        {"kind": "text", "text": "‚Ä¶", "bbox": [x0,y0,x1,y1]},
        {"kind": "table", "rows": [["A","B"],["C","D"]], "bbox": [ ‚Ä¶ ]},
        {"kind": "figure", "caption": "‚Ä¶", "bbox": [ ‚Ä¶ ]}
      ]
    }
  ],
  "meta": {"mime":"application/pdf","page_count":1,"source_path":"docs/x.pdf"}
}
```

**Flags √∫tiles:** *(se configuran en el parser internamente; no hay flags CLI aqu√≠)*

**Notas:**

* Si planeas OCR, instala Tesseract y su pack de idioma (es/en).
* IR es **la base de verdad** para el resto de etapas; si algo se ve raro aqu√≠, arrastrar√° ruido despu√©s.

---

### 2) HybridChunker (IR ‚Üí Chunks)

**Entrada:** `DocumentIR`
**Salida:** `DocumentChunks` (`outputs_chunks/{DOC}_chunks.json`)

**Qu√© hace:**

1. **Aplana** la IR a una secuencia `{kind, text, page_idx, block_idx}`.

   * `heading` se pasa a texto con prefijo `#` para conservar jerarqu√≠a (y se puede **pegar** con el siguiente bloque).
   * `table` se **serializa a texto** estilo CSV por filas (conserva contenido).
2. **Empaqueta** en chunks *cohesivos* con un objetivo de longitud (`target_chars`), sin superar `max_chars`.
3. Si un chunk excede, **corta por oraci√≥n** usando spaCy si hay modelo; si no, **regex robusta**.
4. Aplica **solapamiento** (`overlap`) en caracteres para dar **contexto continuo**.
5. Etiqueta cada chunk como `text` / `table` / `mixed`.

**Contrato de salida (resumen):**

```json
{
  "doc_id": "DOC-XXXX",
  "chunks": [
    {
      "chunk_id": 0,
      "text": "‚Ä¶",
      "kind": "mixed",
      "meta": {
        "chars": 1042, "overlap": 120,
        "page_span": [0,0], "block_span": [3,7]
      }
    }
  ]
}
```

**Flags clave:**

* `--target-chars` (‚âà1400 recomendado), `--max-chars` (2048), `--min-chars` (400), `--overlap` (120).
* `--sentence-splitter {auto|spacy|regex}` (solo afecta cortes internos).
* `--table-policy {isolate|merge}`: mantener tablas separadas o fusionarlas cuando convenga.

**Consejos:**

* Si ‚Äúrompe‚Äù demasiado los p√°rrafos, baja `target_chars` o usa `spacy` en `sentence_splitter`.
* Si tienes muchos cuadros/tablas, prueba `table_policy=merge` para evitar chunks min√∫sculos.

---

### 3) Sentence/Filter (Chunks ‚Üí Sentences)

**Entrada:** `DocumentChunks`
**Salida:** `DocumentSentences` (`outputs_sentences/{DOC}_sentences.json`)

**Qu√© hace:**

1. **Normaliza**: colapsa espacios, repara guiones de l√≠nea, elimina bullets.
2. **Divide en oraciones**: spaCy si est√° disponible; si no, **regex** (dise√±ada para no romper abreviaturas comunes).
3. **Filtra ruido**:

   * `min_chars` (descarta oraciones telegr√°ficas),
   * `drop_stopword_only` y `drop_numeric_only`,
   * **dedupe**: `fuzzy` con `fuzzy_threshold` (evita repetir oraciones near-duplicadas).
4. **Traza** cada oraci√≥n al chunk origen (y por transitividad a la IR).

**Contrato de salida (resumen):**

```json
{
  "doc_id": "DOC-XXXX",
  "sentences": [
    {"text":"‚Ä¶","chunk_id":0,"span":[s,e]}
  ],
  "meta": {
    "counters": {
      "total_split": 42, "kept": 28,
      "dropped_short": 9, "dropped_numeric": 2,
      "dropped_dupe": 3, "keep_rate": 0.67
    }
  }
}
```

**Flags clave:**

* `--sentence-splitter {auto|spacy|regex}`, `--min-chars 25`, `--dedupe fuzzy`, `--fuzzy-threshold 0.92`.
* Toggles: `--no-normalize-whitespace`, `--no-dehyphenate`, `--no-strip-bullets`, `--keep-stopword-only`, `--keep-numeric-only`.

**Consejos:**

* `keep_rate` entre **0.6‚Äì0.9** suele ser sano; muy bajo = filtros agresivos, muy alto = ruido.
* Si salen oraciones ‚Äúcortadas‚Äù, fuerza `--sentence-splitter spacy`.

---

### 4) Triples (Sentences ‚Üí Triples)

**Entrada:** `DocumentSentences`
**Salida:** `DocumentTriples` (`outputs_triples/{DOC}_triples.json`)

**Qu√© hace:**

* Extrae **(Sujeto, Relaci√≥n, Objeto)** con un enfoque biling√ºe **ES/EN**:

  * **spaCy (si disponible)**: reglas de dependencias (SVO, copulares, preposicionales, nominal+prep, apposici√≥n ‚Üí `alias`, pasiva de adquisici√≥n, empleo/cargo).
  * **Regex de respaldo** si no hay spaCy o el √°rbol no ayuda (copulares, adquisici√≥n, empleo, prep gen√©ricas).
* **Anti-ruido**: ignora l√≠neas tipo contacto (URLs, emails, tel√©fonos), headings `#`, cadenas con casi solo n√∫meros/puntuaci√≥n.
* **Canonicaliza** relaciones (opcional): **superficies ES/EN** ‚Üí **forma com√∫n** (p. ej., `trabaja_en` / `works at` ‚Üí `works_at`).
* Asigna **confianza `conf`** por regla (regex gen√©ricas ‚âà0.60; dependencias mayores).

**Contrato de salida (resumen):**

```json
{
  "doc_id": "DOC-XXXX",
  "triples": [
    {
      "subject":"Alice","relation":"works_at","object":"ACME",
      "meta":{
        "dep_rule":"VERB_prep_pobj_nsubj",
        "rel_surface":"works at",
        "conf":0.74,"lang":"en",
        "sentence_idx":12,"span":[s,e]
      }
    }
  ],
  "meta":{"counters":{"used_sents": 3}}
}
```

**Flags clave:**

* `--spacy {auto|force|off}` (fuerza dependencias o usa solo regex),
* `--min-conf-keep 0.66` (recomendado),
* `--max-triples-per-sentence 4`,
* `--lang {auto|es|en}`,
* `--no-canonicalize-relations` (si quieres conservar la superficie textual).

**Consejos:**

* Si ves demasiadas relaciones gen√©ricas de preposici√≥n, **sube** `--min-conf-keep` a 0.70‚Äì0.72 o usa `--spacy force`.
* Para auditor√≠a, une por `sentence_idx` con `DocumentSentences` y revisa `meta.rel_surface`.

---

### 5) Mentions (Sentences ‚Üí Mentions) ‚Äî **NER/RE + consenso con Triples**

**Entrada:** `DocumentSentences` (+ opcionalmente `DocumentTriples` para boost)
**Salida:** `DocumentMentions` (`outputs_mentions/{DOC}_mentions.json`)

**Qu√© hace:**

* Detecta **entidades** (regex/phrase/spaCy) y **relaciones** (DependencyMatcher/regex).
* **Consenso/boost** contra Triples existentes:

  * Si pasas `--boost-from-triples` (o hay `outputs_triples/*_triples.json`), cualquier relaci√≥n mencionada en la **misma oraci√≥n** y con la **misma relaci√≥n** recibe un **incremento de confianza** (`boost_conf`), escalado por `triples_boost_weight`.
* **HF (opcional, offline):** re-ranker local si pones `--use-transformers` y `--hf-rel-model-path` a una carpeta con un modelo de clasificaci√≥n de relaciones. Esto **no descarga** nada.
* Aplica filtros: `min_conf_keep`, `max_relations_per_sentence`, `canonicalize_labels`.

**Contrato de salida (resumen):**

```json
{
  "doc_id":"DOC-XXXX",
  "entities":[
    {"id":"E0","text":"Alice","label":"PERSON","canonical_label":"person","span":[s,e],"conf":0.91}
  ],
  "relations":[
    {
      "subj_entity_id":"E0","obj_entity_id":"E1",
      "label":"works_at","canonical_label":"works_at",
      "sentence_idx":12,"conf":0.78
    }
  ]
}
```

**Flags clave:**

* `--min-conf-keep 0.66`, `--max-relations-per-sentence 6`, `--canonicalize-labels`.
* `--boost-from-triples "outputs_triples/*_triples.json"`, `--boost-conf 0.05‚Äì0.12`, `--triples-boost-weight 1.0`.
* `--use-transformers`, `--hf-rel-model-path`, `--hf-device`, `--hf-batch-size`, `--hf-min-prob`, `--transformer-weight`.

**Consejos:**

* Si activas HF, **aseg√∫rate** de que `hf_rel_model_path` apunte a un directorio local v√°lido. Si no, d√©jalo apagado.
* La m√©trica **Support\@Triples** (en `tools/validate_ie.py`) te dice qu√© % de relaciones en Mentions est√°n soportadas por Triples en la misma oraci√≥n.

---

### 6) IE (Orquestador por documento) ‚Äî **recomendado**

**Entrada:** `DocumentSentences` (o `chunks` si quieres que genere oraciones)
**Salida:** `DocumentTriples` + `DocumentMentions` (carpetas de salida)

**Qu√© hace:**

1. **Resuelve oraciones:**

   * Si das `--sents-glob`, usa esas oraciones.
   * Si no y pasas `--chunks-glob`, **genera** `DocumentSentences`.
   * Si no das nada, busca `outputs_sentences/*_sentences.json`.
2. **Triples**: si **no existen** (o limpiaste), los **genera**; si existen, los **reusa**.
3. **Mentions**: ejecuta NER/RE con **boost** desde los Triples producidos/encontrados.
4. **Validaci√≥n opcional**: `--validate` corre `tools/validate_ie.py` y reporta **Support\@Triples**.

**Flags clave (las que realmente usar√°s):**

* Fuentes/destinos: `--sents-glob`, `--chunks-glob`, `--outdir-sentences`, `--outdir-triples`, `--outdir-mentions`.
* Calidad: `--min-conf-keep-triples`, `--min-conf-keep-mentions`, `--max-relations-per-sentence`, `--canonicalize-labels`, `--boost-conf`.
* HF (opcional): `--use-transformers`, `--hf-rel-model-path`, `--hf-device`.
* Ejecuci√≥n: `--workers`, `--validate`, `--clean-outdir-mentions`, `--clean-outdir-triples`.

**Ejemplo t√≠pico (sin HF):**

```bash
python t2g_cli.py ie \
  --sents-glob "outputs_sentences/*_sentences.json" \
  --outdir-triples outputs_triples \
  --outdir-mentions outputs_mentions \
  --boost-conf 0.08 \
  --validate
```

**Consejos:**

* Si solo quieres **ajustar thresholds de Mentions** sin recalcular Triples, corre `ie` con `--clean-outdir-mentions` y deja Triples intactos.
* Si tus documentos son muy telegr√°ficos, baja `--min-conf-keep-mentions` a 0.55‚Äì0.60 y usa un `boost_conf` un poco m√°s alto (0.10‚Äì0.12).

---

**Checklist mental r√°pido**

* ¬øIR se ve bien? ‚Üí S√≠: sigue. No: reintenta Parser u OCR.
* ¬øChunks dentro de \[400, 2048] con overlap ‚âà120? ‚Üí S√≠.
* ¬øSentences con `keep_rate` razonable (0.6‚Äì0.9)? ‚Üí S√≠.
* ¬øTriples con `min-conf-keep ‚â• 0.66` y relaciones can√≥nicas √∫tiles? ‚Üí S√≠.
* ¬øMentions con buen **Support\@Triples** (p. ej. ‚â•60‚Äì80% seg√∫n dominio)? ‚Üí S√≠.

---


## ‚öôÔ∏è Instalaci√≥n

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# OCR (si planeas usar fallback OCR en PDF y/o IMG)
# macOS:  brew install tesseract tesseract-lang
# Ubuntu: sudo apt install tesseract-ocr tesseract-ocr-spa

# (Opcional) spaCy para mejores cortes y dependencias (Triples)
pip install spacy
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

---

## ‚öôÔ∏è Instalaci√≥n

> Recomendado: **Python 3.11**, entorno virtual, y (si vas a usar HF/torch) fijar `numpy<2` para evitar choques ABI.

```bash
# 1) Crear y activar venv
python3 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate

# 2) Pip moderno
python -m pip install --upgrade pip wheel setuptools

# 3) Instalar dependencias del proyecto
pip install -r requirements.txt
```

### OCR (opcional pero √∫til en PDF/IMG sin texto)

```bash
# macOS (Homebrew)
brew install tesseract tesseract-lang

# Ubuntu/Debian
sudo apt update
sudo apt install tesseract-ocr tesseract-ocr-spa

# Windows (choco)
choco install tesseract
```

### spaCy (opcional; mejora cortes y dependencias para Triples/Mentions)

```bash
pip install spacy
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

### (Opcional) PyTorch + Transformers para re-rank local en Mentions

> Si **no** vas a usar re-rank HF, puedes saltarte esta secci√≥n y dejar `--use-transformers` desactivado.

**CPU / macOS (MPS):**

```bash
pip install "torch>=2.1,<2.3" transformers>=4.38,<4.43
```

**GPU NVIDIA (CUDA 12.1, ejemplo):**

```bash
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio
pip install "transformers>=4.38,<4.43"
```

**Nota de compatibilidad (NumPy):**
Si ves el error *‚ÄúA module that was compiled using NumPy 1.x cannot be run in NumPy 2.x‚Äù*, fija:

```bash
pip install "numpy<2" --upgrade
```

**Modelo HF offline (si decides activarlo):**

```bash
# Descarga un modelo de clasificaci√≥n de relaciones a una carpeta local
# (ejemplo ilustrativo ‚Äî sustituye <repo_id>)
huggingface-cli snapshot-download <repo_id> \
  --local-dir hf_plugins/re_models/relation_mini

# Luego lo habilitas en CLI con:
# --use-transformers --hf-rel-model-path "hf_plugins/re_models/relation_mini"
```

---

## üöÄ Uso r√°pido (CLI)

> Puedes correr etapas sueltas o usar el **orquestador `ie`** (recomendado) o el **pipeline declarativo YAML**.

### 1) Parse ‚Üí IR

Convierte documentos a **IR JSON**.

```bash
python t2g_cli.py parse docs/Resume\ Martin\ Jurado_CDAO_24.pdf --outdir outputs_ir
# Resultado: outputs_ir/DOC-XXXX.json
```

### 2) IR ‚Üí Chunks

Crea chunks sem√°nticos con solapamiento.

```bash
python t2g_cli.py chunk outputs_ir/DOC-XXXX.json --outdir outputs_chunks \
  --sentence-splitter auto --target-chars 1400 --max-chars 2048 --overlap 120
# Resultado: outputs_chunks/DOC-XXXX_chunks.json
```

### 3) Chunks ‚Üí Sentences

Split por oraciones + filtros/normalizaci√≥n.

```bash
python t2g_cli.py sentences outputs_chunks/DOC-XXXX_chunks.json \
  --outdir outputs_sentences \
  --sentence-splitter auto \
  --min-chars 25 \
  --dedupe fuzzy \
  --fuzzy-threshold 0.92
# Resultado: outputs_sentences/DOC-XXXX_sentences.json
```

### 4) Sentences ‚Üí Triples

Triples (S,R,O) biling√ºes con reglas de dependencias/regex.

```bash
python t2g_cli.py triples outputs_sentences/DOC-XXXX_sentences.json \
  --outdir outputs_triples \
  --lang auto --ruleset default-bilingual \
  --spacy auto --max-triples-per-sentence 4 \
  --min-conf-keep 0.66
# Tip: a√±ade --no-canonicalize-relations si quieres conservar superficie cruda.
# Resultado: outputs_triples/DOC-XXXX_triples.json
```

### 5) Sentences ‚Üí Mentions (NER/RE + consenso con Triples)

Si ya tienes triples y quieres iterar r√°pido en NER/RE:

```bash
python t2g_cli.py mentions outputs_sentences/DOC-XXXX_sentences.json \
  --outdir outputs_mentions \
  --lang auto --spacy auto \
  --min-conf-keep 0.66 \
  --max-relations-per-sentence 6 \
  --boost-from-triples "outputs_triples/*_triples.json" \
  --boost-conf 0.08
# Opcional HF:
# --use-transformers --hf-rel-model-path "hf_plugins/re_models/relation_mini" --hf-device cpu
# Resultado: outputs_mentions/DOC-XXXX_mentions.json
```

### 6) IE ‚Äî Orquestador por documento (recomendado)

Hace: **(reusa o crea) Triples ‚Üí Mentions con boost**.
Si ya corriste `sentences`, esta es la forma m√°s pr√°ctica:

```bash
python t2g_cli.py ie \
  --sents-glob "outputs_sentences/*_sentences.json" \
  --outdir-triples outputs_triples \
  --outdir-mentions outputs_mentions \
  --boost-conf 0.08 \
  --validate
# Limpia menciones para iterar thresholds sin recalcular triples:
#   a√±ade: --clean-outdir-mentions
# Apaga spaCy (regex-only): --spacy off
# Activa HF offline:
#   --use-transformers --hf-rel-model-path "hf_plugins/re_models/relation_mini" --hf-device cpu
```

### 7) Pipeline declarativo (YAML)

Corre todo el flujo desde `pipelines/pipeline.yaml`:

```bash
python t2g_cli.py pipeline-yaml
# o
python t2g_cli.py pipeline-yaml --file pipelines/pipeline.yaml
```

---

### Verificaci√≥n r√°pida de salidas

```bash
# Revisa contadores
jq '.meta.counters // {}' outputs_sentences/*_sentences.json | head
jq '.meta.counters // {}' outputs_triples/*_triples.json | head
jq '.meta.counters // {}' outputs_mentions/*_mentions.json | head

# Valida soporte Triples‚ÜíMentions
python tools/validate_ie.py --mentions outputs_mentions --triples outputs_triples
```

**Consejos finales:**

* Si ves demasiadas relaciones ‚Äúgen√©ricas‚Äù, sube `--min-conf-keep` o usa `--spacy force`.
* Si los documentos son telegr√°ficos, baja `--min-conf-keep` en Mentions (0.55‚Äì0.60) y usa `--boost-conf 0.10‚Äì0.12`.
* Si activas HF y no hay modelo local v√°lido, el sistema sigue corriendo **sin** HF (log de advertencia).
## üßæ Pipeline declarativo (YAML)



Archivo por defecto: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: chunk
    args:
      clean_outdir: true
      ir_glob: "outputs_ir/*.json"
      outdir: "outputs_chunks"
      target_chars: 1400
      max_chars: 2048
      min_chars: 400
      overlap: 120
      table_policy: "isolate"
      sentence_splitter: "auto"

  - name: sentences
    args:
      clean_outdir: true
      chunks_glob: "outputs_chunks/*_chunks.json"
      outdir: "outputs_sentences"
      sentence_splitter: "auto"
      min_chars: 25
      dedupe: "fuzzy"
      fuzzy_threshold: 0.92
      no_normalize_whitespace: false
      no_dehyphenate: false
      no_strip_bullets: false
      keep_stopword_only: false
      keep_numeric_only: false

  # IE orquestador (recomendado)
  - name: ie
    args:
      sents_glob: "outputs_sentences/*_sentences.json"
      outdir_triples: "outputs_triples"
      outdir_mentions: "outputs_mentions"
      lang: "auto"
      spacy: "auto"
      min_conf_keep_triples: 0.66
      min_conf_keep_mentions: 0.66
      max_relations_per_sentence: 6
      canonicalize_labels: true
      boost_conf: 0.08
      use_transformers: false         # true si tienes modelo HF local
      # hf_rel_model_path: "hf_plugins/re_models/relation_mini"
      # hf_device: "cpu"
      workers: 4
      validate: true
      clean_outdir_triples: false
      clean_outdir_mentions: true
```

> **Alternativas avanzadas:**
> ‚Ä¢ Ejecutar **Triples** y **Mentions** como etapas independientes (√∫til para auditar o ajustar thresholds sin recalcular).
> ‚Ä¢ Si omites `sentences`, puedes pasar `chunks_glob` al `ie` para que genere oraciones internamente.

---
Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
# o expl√≠cito:
python t2g_cli.py pipeline-yaml --file pipelines/pipeline.yaml
```

---

## üìä M√©tricas por subsistema

### Parser

* **`percent_docs_ok`**: % de documentos parseados sin error (√©xito de lote).
* **`layout_loss`**: proporci√≥n `unknown/total` (proxy de p√©rdida de estructura).
* **`table_consistency`**: tablas encontradas vs. valor *golden* (si existe).

**Umbrales sugeridos:** `%docs_ok ‚â• 95%`, `layout_loss ‚â§ 0.15`, `table_consistency.ratio ‚â• 0.9`.

```python
import json, glob
from parser.schemas import DocumentIR
from parser.metrics import percent_docs_ok, layout_loss, table_consistency

irs = []
for path in sorted(glob.glob("outputs_ir/*.json")):
    try:
        irs.append(DocumentIR(**json.load(open(path, "r", encoding="utf-8"))))
    except Exception:
        continue

ok_pct = percent_docs_ok(irs)
losses = {ir.doc_id: layout_loss(ir) for ir in irs}
cons   = {ir.doc_id: table_consistency(ir) for ir in irs}

print({"percent_docs_ok": round(ok_pct, 2)})
print({"layout_loss_avg": round(sum(losses.values())/max(1,len(losses)), 4)})
print({"table_consistency_sample": list(cons.items())[:2]})
```

---

### HybridChunker

* **`chunk_length_stats`**: distribuci√≥n de longitudes.
* **`percent_within_threshold(min,max)`**: proporci√≥n de chunks dentro del rango (p. ej. 400‚Äì2048).
* **`table_mix_ratio`**: proporci√≥n `text/mixed/table`.

**Umbrales sugeridos:** `within(400‚Äì2048) ‚â• 0.95`.

```python
import json, glob
from chunker.schemas import DocumentChunks
from chunker.metrics import chunk_length_stats, percent_within_threshold, table_mix_ratio

for path in sorted(glob.glob("outputs_chunks/*_chunks.json"))[:3]:
    dc = DocumentChunks(**json.load(open(path, "r", encoding="utf-8")))
    print(dc.doc_id, {
        "len_stats": chunk_length_stats(dc),
        "within_400_2048": percent_within_threshold(dc, 400, 2048),
        "mix": table_mix_ratio(dc)
    })
```

---

### Sentence/Filter

* **`meta.counters`** en salida: `total_split`, `kept`, `dropped_short`, `dropped_stopword`, `dropped_numeric`, `dropped_dupe`, `keep_rate`.
* **`sentence_length_stats`**: distribuci√≥n de longitudes de oraciones.
* **`unique_ratio`**: proporci√≥n de oraciones √∫nicas (detecta duplicados).

**Umbrales sugeridos:** `keep_rate` saludable ‚âà 0.6‚Äì0.9 seg√∫n dominio; `unique_ratio ‚â• 0.85`.

```python
import json, glob
from sentence_filter.schemas import DocumentSentences
from sentence_filter.metrics import sentence_length_stats, unique_ratio

for path in sorted(glob.glob("outputs_sentences/*_sentences.json"))[:3]:
    ds = DocumentSentences(**json.load(open(path, "r", encoding="utf-8")))
    counters = ds.meta.get("counters", {})
    print(ds.doc_id, {
        "keep_rate": round(counters.get("keep_rate", 0), 3),
        "kept": counters.get("kept"),
        "len_stats": sentence_length_stats(ds),
        "unique_ratio": round(unique_ratio(ds), 3)
    })
```

---

### Triples

* **Agregados:** n¬∫ de documentos, **n¬∫ de triples**, **unique ratio** (`drop_duplicates` por S,R,O).
* **Distribuci√≥n de relaciones** y **reglas** (qu√© patrones producen qu√©).
* **`conf` stats** y efecto de `--min-conf-keep`.
* **Auditor√≠a con contexto:** join contra oraciones por `sentence_idx`.

```python
import json, glob, pandas as pd
from triples.schemas import DocumentTriples

rows = []
for p in sorted(glob.glob("outputs_triples/*_triples.json")):
    dt = DocumentTriples(**json.load(open(p)))
    for t in dt.triples:
        rows.append({
          "doc_id": dt.doc_id,
          "subject": t.subject, "relation": t.relation, "object": t.object,
          "dep_rule": t.meta.get("dep_rule"), "conf": t.meta.get("conf"),
          "lang": t.meta.get("lang"), "sentence_idx": t.meta.get("sentence_idx"),
          "file": p
        })

df = pd.DataFrame(rows)
print("Docs:", df["doc_id"].nunique(), "| Triples:", len(df))
print("Unique ratio:", df.drop_duplicates(["subject","relation","object"]).shape[0]/max(1,len(df)))
print("Top relaciones:\n", df["relation"].value_counts().head())
print("Top reglas:\n", df["dep_rule"].value_counts().head())
print("Conf stats:\n", df["conf"].describe())
```

### **Mentions (NER/RE)**

**Agregados:**

* N¬∫ de entidades y relaciones por doc.
* Distribuci√≥n por etiquetas (`entity.label`, `relation.canonical_label`).
* **Soporte por Triples**: % de relaciones de Mentions con pareja `(doc_id, sentence_idx, relation)` presente en Triples.

**Snippet (soporte vs Triples):**

```python
import json, glob, pandas as pd

# Carga Mentions
mrs = []
for p in sorted(glob.glob("outputs_mentions/*_mentions.json")):
    dm = json.load(open(p, "r", encoding="utf-8"))
    for r in dm.get("relations", []):
        mrs.append({
          "doc_id": dm.get("doc_id"),
          "sentence_idx": r.get("sentence_idx"),
          "label": r.get("canonical_label") or r.get("label") or "",
          "subj": r.get("subj_entity_id"),
          "obj": r.get("obj_entity_id"),
          "conf": r.get("conf", 0.0),
          "file": p
        })
rel_df = pd.DataFrame(mrs)

# Carga Triples
trs = []
for p in sorted(glob.glob("outputs_triples/*_triples.json")):
    dt = json.load(open(p, "r", encoding="utf-8"))
    for t in dt.get("triples", []):
        trs.append({
          "doc_id": dt.get("doc_id"),
          "sentence_idx": (t.get("meta", {}) or {}).get("sentence_idx"),
          "relation": t.get("relation") or "",
          "file": p
        })
tri_df = pd.DataFrame(trs)

# Keys y soporte
if not rel_df.empty and not tri_df.empty:
    rel_df["key"] = rel_df.apply(lambda r: f"{r['doc_id']}::{int(r['sentence_idx'])}::{str(r['label']).lower()}", axis=1)
    tri_df["key"] = tri_df.apply(lambda r: f"{r['doc_id']}::{int(r['sentence_idx'])}::{str(r['relation']).lower()}", axis=1)
    tri_keys = set(tri_df["key"].tolist())
    rel_df["supported"] = rel_df["key"].isin(tri_keys)
    print("Support@Triples:", f"{rel_df['supported'].mean()*100:.2f}%", f"({rel_df['supported'].sum()}/{len(rel_df)})")
    print(rel_df.groupby(["label","supported"]).size().unstack(fill_value=0).head())
else:
    print("No hay datos suficientes para calcular soporte.")
```

**Export a grafo (opcional):**

```python
import json, glob, networkx as nx

G = nx.MultiDiGraph()
for p in sorted(glob.glob("outputs_mentions/*_mentions.json")):
    dm = json.load(open(p, "r", encoding="utf-8"))
    ents = {e["id"]: e for e in dm.get("entities", [])}
    for r in dm.get("relations", []):
        s = ents.get(r["subj_entity_id"]); o = ents.get(r["obj_entity_id"])
        if not s or not o: continue
        G.add_node(s["id"], label=s.get("canonical_label") or s.get("label"), text=s["text"])
        G.add_node(o["id"], label=o.get("canonical_label") or o.get("label"), text=o["text"])
        G.add_edge(s["id"], o["id"], r=r.get("canonical_label") or r.get("label"), conf=r.get("conf", 0.0))
nx.write_gexf(G, "outputs_metrics/mentions_graph.gexf")
print("Grafo exportado a outputs_metrics/mentions_graph.gexf")
```

**Auditor√≠a con oraci√≥n original:**

```python
import json, pandas as pd

doc = "DOC-XXXX"
dt = json.load(open(f"outputs_triples/{doc}_triples.json"))
ds = json.load(open(f"outputs_sentences/{doc}_sentences.json"))

trip = pd.json_normalize(dt["triples"])
trip["sentence_idx"] = trip["meta.sentence_idx"]
sents = pd.DataFrame(ds["sentences"]).assign(sentence_idx=lambda d: range(len(d)))
audit = trip.merge(sents[["sentence_idx","text"]], on="sentence_idx", how="left")

cols = ["subject","relation","object","meta.conf","meta.dep_rule","text"]
print(audit[cols].head(12))
```

**Sugerencias de calidad:**

* Usa `--min-conf-keep 0.66` para recortar `regex_prep` gen√©ricas.
* Si hay ruido de preposiciones, **sube** a 0.70‚Äì0.72 o activa spaCy (`--spacy force`) para m√°s precisi√≥n estructural.
* Desactiva `--no-canonicalize-relations` si necesitas la superficie cruda (√∫til para depuraci√≥n).

---

## üõ†Ô∏è Troubleshooting

* **JSONs vac√≠os/corruptos:** escritura **at√≥mica**, el runner **salta** JSON inv√°lidos. Limpia y re-ejecuta:

  ```bash
  rm -rf outputs_ir/* outputs_chunks/* outputs_sentences/* outputs_triples/*
  python t2g_cli.py pipeline-yaml
  ```
* **spaCy no carga / modelos faltantes:** instala `es_core_news_sm` y `en_core_web_sm` o usa `--spacy off` (regex-only).
* **OCR fallback no funciona:** instala Tesseract y verifica `PATH` (o `tesseract_cmd` en Windows).
* **Imports fallan:** ejecuta desde la **ra√≠z** (`python t2g_cli.py ‚Ä¶`).
* **Triples con ‚Äúof/in/with‚Äù dominantes:** eleva `--min-conf-keep` o fuerza spaCy.
* **Pandas `InvalidIndexError` en auditor√≠as:** usa `reset_index(drop=True)` antes de `concat/merge`.
* **Carpetas con ‚Äúruido‚Äù de corridas previas:** en YAML pon `clean_outdir: true` en cada etapa.

---

## üß™ Roadmap inmediato

* NER/RE y normalizaci√≥n (fechas, montos, IDs).
* Publicaci√≥n a ES/Qdrant/Postgres/Grafo (con esquemas para triples).
* M√©tricas de evaluaci√≥n y lazo humano (RAGAS + HITL).
* Tests (`pytest`) y *golden sets* de regresi√≥n.

---

## üìö Referencias

* **OCR:** PaddleOCR, Tesseract
* **PDF:** pdfplumber
* **DOCX:** python-docx
* **NLP:** spaCy
* **Papers:** DocRED (ACL 2019), K-Adapter (ACL 2020), AutoNER (ACL 2018), KG-BERT (AAAI 2020)

---

## ‚öñÔ∏è Licencia

MIT (o la que definas).
