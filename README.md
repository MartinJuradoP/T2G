# üìö Proyecto T2G ‚Äî Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular* para convertir documentos heterog√©neos (PDF, DOCX, im√°genes) en una **Representaci√≥n Intermedia (IR) homog√©nea**, segmentarlos en **chunks** sem√°nticos y luego en **oraciones filtradas**, listas para RAG/IE/grafos.

* **Entrada:** PDF / DOCX / IMG
* **Salidas (hoy):** **IR (JSON)** ‚Üí **Chunks (JSON)** ‚Üí **Sentences (JSON)**
* **Dise√±o:** subsistemas desacoplados, contratos claros, ejecuci√≥n CLI/YAML

---

## ‚ú® Objetivos

* Convertir documentos heterog√©neos en una **IR homog√©nea JSON** con bloques y tablas.
* Desarrollar, probar y orquestar los **subsistemas**: Parser, Chunker, Sentence/Filter, (NER/RE), Normalizaci√≥n, Publicaci√≥n, Retriever, Evaluaci√≥n.
* Sentar base para **grafos de conocimiento**, **QA empresarial** y **compliance**.
* Mantener una arquitectura **escalable y modular**, con **contratos Pydantic** y CLIs consistentes.

---

## üß© Subsistemas (vivos y planeados)

| N¬∫ | Subsistema          | Rol                                                      | I/O                                     | Estado |
| -: | ------------------- | -------------------------------------------------------- | --------------------------------------- | ------ |
|  1 | **Parser**          | Unificar formatos a **IR JSON/MD** con layout y tablas   | Doc ‚Üí **IR**                            | ‚úÖ      |
|  2 | **HybridChunker**   | Chunks **cohesivos** con tama√±os estables y solapamiento | IR ‚Üí **Chunks**                         | ‚úÖ      |
|  3 | **Sentence/Filter** | Dividir en **oraciones** y filtrar ruido antes de IE     | Chunks ‚Üí **Sentences**                  | ‚úÖ      |
|  4 | Triples (dep.)      | (S,R,O) ligeros (dependency-based)                       | Sentences ‚Üí Triples                     | üïí     |
|  5 | Extracci√≥n (NER/RE) | Entidades y relaciones                                   | Sentences ‚Üí Menciones                   | üïí     |
|  6 | Normalizaci√≥n       | Fechas, montos, IDs, orgs                                | Menciones ‚Üí Entidades                   | üïí     |
|  7 | Publicaci√≥n         | √çndices / grafo 1-hop                                    | Chunks/Ent/Triples ‚Üí ES/Qdrant/PG/Grafo | üïí     |
|  8 | Retriever (cascada) | Recall ‚Üí Precisi√≥n                                       | Query ‚Üí Contexto                        | üïí     |
|  9 | Evaluaci√≥n & HITL   | Calidad / drift / lazo humano                            | Respuestas ‚Üí Scores                     | üïí     |

---

## üìÇ Estructura del proyecto

```
project_T2G/
‚îú‚îÄ‚îÄ docs/                      # Documentos de prueba (PDF, DOCX, PNG, JPG)
‚îú‚îÄ‚îÄ parser/                    # Subsistema 1: Parser
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ chunker/                   # Subsistema 2: HybridChunker
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_chunker.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ sentence_filter/           # Subsistema 3: Sentence/Filter
‚îÇ   ‚îú‚îÄ‚îÄ sentence_filter.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.yaml          # Pipeline declarativo (YAML)
‚îú‚îÄ‚îÄ outputs_ir/                # Salidas IR (JSON)
‚îú‚îÄ‚îÄ outputs_chunks/            # Salidas Chunks (JSON)
‚îú‚îÄ‚îÄ outputs_sentences/         # Salidas Sentences (JSON)
‚îú‚îÄ‚îÄ t2g_cli.py                 # CLI unificado (parse, chunk, sentences, pipeline-yaml)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> **Nota:** aseg√∫rate de que `docs/` exista; ah√≠ van tus archivos de prueba. Ejemplos t√≠picos:
>
> ```
> docs/
> ‚îú‚îÄ‚îÄ Resume Martin Jurado_CDAO_24.pdf
> ‚îú‚îÄ‚îÄ tabla.png
> ‚îî‚îÄ‚îÄ ejemplo.docx
> ```

---

## üß† Qu√© hace cada etapa

### Parser (Doc ‚Üí IR)

Convierte PDF/DOCX/IMG a una **IR homog√©nea** con bloques de texto y tablas:

* **Detecci√≥n de tipo** por MIME/extensi√≥n y env√≠o a parser especializado.
* **PDF (pdfplumber):**
  texto por l√≠neas, tablas b√°sicas (`vertical/horizontal_strategy=lines`) y **fallback OCR** (Tesseract) por p√°gina si no hay texto/tabla.
* **DOCX (python-docx):**
  p√°rrafos, *headings* por estilo, tablas por celda; devuelve una **p√°gina l√≥gica**.
* **IMG (Pillow + Tesseract):**
  OCR directo con normalizaci√≥n.
* **Normalizaci√≥n:** `normalize_whitespace`, `dehyphenate`.
* **Metadatos:** `size_bytes`, `page_count`, `mime`, `source_path`, `created_at`.

**Contrato (IR):**
`DocumentIR.pages[*].blocks` puede contener **dicts** (r√°pidos) o **modelos Pydantic** (`TextBlock`, `TableBlock`, `FigureBlock`). Campos de layout/OCR/provenance est√°n listos para enriquecerse luego.

---

### HybridChunker (IR ‚Üí Chunks)

Genera **chunks sem√°nticos** ‚â§ `max_chars`, respetando l√≠mites naturales y a√±adiendo **solapamiento**:

1. **Aplanado** IR a unidades `{kind, text, page_idx, block_idx}`:

   * `heading` ‚Üí prefijos `#` para conservar jerarqu√≠a (opci√≥n para **pegar** con siguiente bloque).
   * `paragraph` ‚Üí texto limpio (normalizado desde Parser).
   * `table` ‚Üí **serializaci√≥n CSV-like** por filas a texto plano.
2. **Empaquetado codicioso**:

   * acumula hasta `target_chars`; si cabe, empuja 1 unidad m√°s sin exceder `max_chars`.
   * si excede, **corta por oraci√≥n** con *spaCy* (si disponible) o **regex**.
3. **Solapamiento** (`overlap_chars`) para continuidad.
4. **Clasificaci√≥n** por contenido: `table`, `text`, `mixed`.

**Flags clave:** `target_chars`, `max_chars`, `min_chars`, `overlap`, `sentence_splitter` (`auto|spacy|regex`), `table_policy` (`isolate|merge`).

---

### Sentence/Filter (Chunks ‚Üí Sentences)

Divide **chunks** en **oraciones** y filtra ruido:

1. **Normaliza** (espacios, guiones partidos, bullets).
2. **Divide en oraciones** con *spaCy* (si est√°) o **regex** (robusta).
3. **Filtra**:

   * `min_chars` (descarta oraciones muy cortas),
   * `drop_stopword_only` (solo stopwords),
   * `drop_numeric_only` (sin letras),
   * `dedupe` (`none|exact|fuzzy`) con umbral `fuzzy_threshold`.
4. **Devuelve trazabilidad** (`chunk_id`, `page_span`, offsets en chunk).

---

## ‚öôÔ∏è Instalaci√≥n

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# OCR (si planeas usar fallback OCR en PDF y/o IMG)
# macOS:  brew install tesseract tesseract-lang
# Ubuntu: sudo apt install tesseract-ocr tesseract-ocr-spa

# (Opcional) spaCy para mejores cortes por oraci√≥n
pip install spacy
python -m spacy download es_core_news_sm
# (ingl√©s opcional)
python -m spacy download en_core_web_sm
```

---

## üöÄ Uso r√°pido (CLI)

### Parse ‚Üí IR

```bash
python t2g_cli.py parse docs/Resume\ Martin\ Jurado_CDAO_24.pdf --outdir outputs_ir
```

### IR ‚Üí Chunks

```bash
python t2g_cli.py chunk outputs_ir/DOC-XXXX.json --outdir outputs_chunks \
  --sentence-splitter spacy --target-chars 1400 --max-chars 2048 --overlap 120
```

### Chunks ‚Üí Sentences

```bash
python t2g_cli.py sentences outputs_chunks/DOC-XXXX_chunks.json \
  --outdir outputs_sentences \
  --sentence-splitter auto --min-chars 25 --dedupe fuzzy --fuzzy-threshold 0.92
```

---

## üßæ Pipeline declarativo (YAML)

Archivo por defecto: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: chunk
    args:
      ir_glob: "outputs_ir/*.json"
      outdir: "outputs_chunks"
      target_chars: 1400
      max_chars: 2048
      min_chars: 400
      overlap: 120
      table_policy: "isolate"    # isolate | merge
      sentence_splitter: "auto"  # auto | spacy | regex

  - name: sentences
    args:
      chunks_glob: "outputs_chunks/*_chunks.json"
      outdir: "outputs_sentences"
      sentence_splitter: "auto"  # o "spacy"
      min_chars: 25
      dedupe: "fuzzy"            # none | exact | fuzzy
      fuzzy_threshold: 0.92
      # toggles
      no_normalize_whitespace: false
      no_dehyphenate: false
      no_strip_bullets: false
      keep_stopword_only: false
      keep_numeric_only: false
```

Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
# o expl√≠cito:
python t2g_cli.py pipeline-yaml --file pipelines/pipeline.yaml
```

---

## üìä M√©tricas por subsistema

### Parser

* **`percent_docs_ok`**: % de documentos parseados sin error (√©xito de lote).
* **`layout_loss`**: proporci√≥n `unknown/total` (proxy de p√©rdida de estructura).
* **`table_consistency`**: tablas encontradas vs. valor *golden* (si existe).

**Umbrales sugeridos:** `%docs_ok ‚â• 95%`, `layout_loss ‚â§ 0.15`, `table_consistency.ratio ‚â• 0.9`.

```python
import json, glob
from parser.schemas import DocumentIR
from parser.metrics import percent_docs_ok, layout_loss, table_consistency

irs = []
for path in sorted(glob.glob("outputs_ir/*.json")):
    try:
        irs.append(DocumentIR(**json.load(open(path, "r", encoding="utf-8"))))
    except Exception:
        continue

ok_pct = percent_docs_ok(irs)
losses = {ir.doc_id: layout_loss(ir) for ir in irs}
cons   = {ir.doc_id: table_consistency(ir) for ir in irs}

print({"percent_docs_ok": round(ok_pct, 2)})
print({"layout_loss_avg": round(sum(losses.values())/max(1,len(losses)), 4)})
print({"table_consistency_sample": list(cons.items())[:2]})
```

---

### HybridChunker

* **`chunk_length_stats`**: distribuci√≥n de longitudes.
* **`percent_within_threshold(min,max)`**: proporci√≥n de chunks dentro del rango (p. ej. 400‚Äì2048).
* **`table_mix_ratio`**: proporci√≥n `text/mixed/table`.

**Umbrales sugeridos:** `within(400‚Äì2048) ‚â• 0.95`.

```python
import json, glob
from parser.schemas import DocumentChunks
from chunker.metrics import chunk_length_stats, percent_within_threshold, table_mix_ratio

for path in sorted(glob.glob("outputs_chunks/*_chunks.json"))[:3]:
    dc = DocumentChunks(**json.load(open(path, "r", encoding="utf-8")))
    print(dc.doc_id, {
        "len_stats": chunk_length_stats(dc),
        "within_400_2048": percent_within_threshold(dc, 400, 2048),
        "mix": table_mix_ratio(dc)
    })
```

---

### Sentence/Filter

* **`meta.counters`** en salida: `total_split`, `kept`, `dropped_short`, `dropped_stopword`, `dropped_numeric`, `dropped_dupe`, `keep_rate`.
* **`sentence_length_stats`**: distribuci√≥n de longitudes de oraciones.
* **`unique_ratio`**: proporci√≥n de oraciones √∫nicas (detecta duplicados).

**Umbrales sugeridos:** `keep_rate` saludable ‚âà 0.6‚Äì0.9 seg√∫n dominio; `unique_ratio ‚â• 0.85`.

```python
import json, glob
from sentence_filter.schemas import DocumentSentences
from sentence_filter.metrics import sentence_length_stats, unique_ratio

for path in sorted(glob.glob("outputs_sentences/*_sentences.json"))[:3]:
    ds = DocumentSentences(**json.load(open(path, "r", encoding="utf-8")))
    counters = ds.meta.get("counters", {})
    print(ds.doc_id, {
        "keep_rate": round(counters.get("keep_rate", 0), 3),
        "kept": counters.get("kept"),
        "len_stats": sentence_length_stats(ds),
        "unique_ratio": round(unique_ratio(ds), 3)
    })
```

---

## üõ†Ô∏è Troubleshooting

* **JSONs vac√≠os/corruptos:** se escribe de forma **at√≥mica** y el runner **salta** JSON inv√°lidos. Puedes limpiar y re-ejecutar:

  ```bash
  rm -rf outputs_ir/* outputs_chunks/* outputs_sentences/*
  python t2g_cli.py pipeline-yaml
  ```
* **spaCy no carga:** instala `es_core_news_sm` (y opcionalmente `en_core_web_sm`).
* **OCR fallback no funciona:** instala Tesseract y configura `tesseract_cmd` (Windows) o aseg√∫rate de que est√© en `PATH`.
* **Imports fallan:** ejecuta desde la **ra√≠z** del repo (`python t2g_cli.py ‚Ä¶`).

---

## üß™ Roadmap inmediato

* Triples (dependency-based) y NER/RE.
* Normalizaci√≥n (fechas, montos, IDs) con validadores robustos.
* Publicaci√≥n a ES/Qdrant/Postgres/Grafo.
* M√©tricas de evaluaci√≥n y lazo humano (RAGAS + HITL).
* Tests (`pytest`) y *golden sets* de regresi√≥n.

---

## üìö Referencias

* **OCR:** PaddleOCR, Tesseract
* **PDF:** pdfplumber
* **DOCX:** python-docx
* **NLP:** spaCy
* **Papers:** DocRED (ACL 2019), K-Adapter (ACL 2020), AutoNER (ACL 2018), KG-BERT (AAAI 2020)

---

## ‚öñÔ∏è Licencia

MIT (o la que definas).
