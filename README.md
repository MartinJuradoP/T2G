# üìö Proyecto T2G ‚Äî Knowledge Graph a partir de Documentos

**T2G** es una *pipeline modular y extensible* que convierte documentos heterog√©neos (PDF, DOCX, im√°genes) en una **Representaci√≥n Intermedia (IR) homog√©nea**, los enriquece con **contexto sem√°ntico global y local**, y prepara la base para construir **grafos de conocimiento** y sistemas de **b√∫squeda avanzada (RAG, QA, compliance, etc.)**.

* **Entrada (hoy):** PDF / DOCX / PNG / JPG
* **Salidas (hoy):**

  * `DocumentIR (JSON)`
  * `DocumentIR+Topics (JSON)`
* **Salidas futuras:** Chunks, Mentions, Entities, Triples, Normalizaci√≥n, Grafo.
* **Dise√±o:** subsistemas **desacoplados**, contratos **Pydantic**, orquestaci√≥n v√≠a **CLI + YAML**.

---

## ‚ú® Objetivos

* Unificar la ingesta de documentos en una **IR JSON com√∫n** independientemente del formato.
* Enriquecer documentos con **contexto sem√°ntico a nivel documento y chunk** usando **embeddings + BERTopic**.
* Mantener una arquitectura **resiliente, escalable y modular**: cada subsistema puede ejecutarse de forma independiente.
* Preparar la base para **grafos de conocimiento**, **QA empresarial**, **compliance regulatorio** y **sistemas RAG**.

---

## üß© Subsistemas

| N¬∫ | Subsistema                       | Rol principal                                                               | Entrada             | Salida                   | Estado |
| -: | -------------------------------- | --------------------------------------------------------------------------- | ------------------- | ------------------------ | ------ |
|  1 | **Parser**                       | Genera **IR JSON** homog√©nea con metadatos y layout                         | Doc (PDF/DOCX/IMG)  | `DocumentIR` JSON        | ‚úÖ      |
|  2 | **BERTopic Contextizer (doc)**   | Asigna **t√≥picos y keywords globales** a nivel documento                    | `DocumentIR`        | `DocumentIR+Topics` JSON | ‚úÖ      |
|  3 | **HybridChunker**                | Segmenta documento en **chunks sem√°nticos estables (‚â§2048 tokens)**         | `DocumentIR+Topics` | `DocumentChunks` JSON    | ‚úÖ     |
|  4 | **BERTopic Contextizer (chunk)** | Asigna t√≥picos locales a cada chunk (subtemas); enlaza con t√≥picos globales | `DocumentChunks`    | `Chunks+Topics` JSON     | ‚úÖ     |
|  5 | **Adaptive Schema Selector**     | Define din√°micamente entidades relevantes seg√∫n contexto                    | `Chunks+Topics`     | `SchemaSelection` JSON   | üîú     |
|  6 | **Mentions (NER/RE)**            | Detecta menciones condicionadas por t√≥picos                                 | `Chunks+Topics`     | `Mentions` JSON          | üîú     |
|  7 | **Clustering de Menciones**      | Agrupa spans en clusters sem√°nticos                                         | `Mentions` JSON     | `Clusters` JSON          | üîú     |
|  8 | **Weak Supervision / Label**     | Etiqueta clusters de alta confianza (Snorkel-style)                         | `Clusters`          | `LabeledClusters` JSON   | üîú     |
|  9 | **LLM Intervention**             | Clasifica clusters ambiguos con **few-shot prompting o prototipos**         | `Clusters`          | `RefinedLabels` JSON     | üîú     |
| 10 | **Normalizaci√≥n (h√≠brida)**      | Canonicaliza entidades y estandariza representaciones                       | `Mentions/Clusters` | `Entities` JSON          | üîú     |
| 11 | **Graph Export**                 | Publica entidades y relaciones en grafos (Neo4j, GraphDB, RDF/SHACL)        | `Entities+Triples`  | Grafo / DB               | üîú     |

---

## üìÇ Estructura del proyecto

```bash
project_T2G/
‚îú‚îÄ‚îÄ docs/                      # Documentos de prueba
‚îú‚îÄ‚îÄ parser/                    # Subsistema Parser
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ contextizer/               # Subsistema Contextizer
‚îÇ   ‚îú‚îÄ‚îÄ contextizer.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ utils.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ schema_selector/           # Adaptive Schema Selector üî•
‚îÇ   ‚îú‚îÄ‚îÄ registry.py            # Ontolog√≠as y dominios (medical, legal, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py             # Contratos Pydantic + Config
‚îÇ   ‚îú‚îÄ‚îÄ selector.py            # L√≥gica de scoring y selecci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ utils.py               # Funciones auxiliares (similitud, normalizaci√≥n)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.yaml
‚îú‚îÄ‚îÄ outputs_ir/                # Salidas: DocumentIR
‚îú‚îÄ‚îÄ outputs_doc_topics/        # Salidas: IR+Topics
‚îú‚îÄ‚îÄ outputs_chunks/            # Salidas: Chunks
‚îú‚îÄ‚îÄ outputs_schema/            # Salidas: SchemaSelection
‚îú‚îÄ‚îÄ t2g_cli.py                 # CLI unificado
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üß† Etapas explicadas

---

### 1) Parser (Doc ‚Üí IR) ‚úÖ

**Entrada:** PDF / DOCX / PNG / JPG
**Salida:** `DocumentIR` (`outputs_ir/{DOC}_*.json`)

**Qu√© hace:**

* Detecta formato y selecciona parser:

  * **PDF:** texto + tablas (pdfplumber); OCR fallback si escaneado.
  * **DOCX:** p√°rrafos, headings, tablas (python-docx).
  * **IMG:** OCR (pytesseract).
* Normaliza espacios, guiones cortados, saltos de l√≠nea.
* A√±ade metadatos: `sha256`, `mime`, `page_count`, `size_bytes`.

**Ejemplo de salida:**

```json
{
  "doc_id": "DOC-12345",
  "pages": [
    {
      "page_number": 1,
      "blocks": [
        {"type": "paragraph", "text": "Los leones viven en √Åfrica..."},
        {"type": "table", "cells": [{"row":0,"col":0,"text":"Dato"}]}
      ]
    }
  ],
  "meta": {
    "mime":"application/pdf",
    "page_count":1,
    "sha256":"‚Ä¶",
    "source_path":"docs/leones.pdf"
  }
}
```

---

### 2) BERTopic Contextizer (doc-level) ‚úÖ

**Entrada:** `DocumentIR` (`outputs_ir/*.json`)
**Salida:** `DocumentIR+Topics` (`outputs_doc_topics/*.json`)

**Qu√© hace:**

* **1. Extracci√≥n de texto**:
  Toma todos los bloques textuales del documento (p√°rrafos, headings, tablas OCR).
  Preprocesa eliminando espacios raros, stopwords multiling√ºes y normalizando tokens.

* **2. Embeddings globales**:
  Cada bloque se convierte en un vector usando **SentenceTransformers** (`all-MiniLM-L6-v2`).
  Estos embeddings capturan similitud sem√°ntica m√°s all√° de palabras exactas.

* **3. Clustering de t√≥picos con BERTopic**:
  Los embeddings se reducen con **UMAP** y se agrupan con **HDBSCAN**.

  * **UMAP** ‚Üí baja dimensi√≥n para preservar estructura sem√°ntica.
  * **HDBSCAN** ‚Üí encuentra clusters de tama√±o variable sin fijar `k`.
  * **BERTopic** ‚Üí asigna palabras clave representativas a cada cluster.

* **4. Heur√≠stica adaptativa (corpus peque√±o)**:
  Como documentos pueden ser muy cortos, aplicamos **fallbacks** para evitar errores o ruido:

  * **0‚Äì1 bloques** ‚Üí se asigna un √∫nico topic (`singleton`), con keywords derivadas por frecuencia.
  * **2 bloques** ‚Üí no hay suficiente masa para clustering; se aplica **TF-based fallback** (frecuencia de t√©rminos relevantes).
  * **‚â•3 bloques** ‚Üí se ejecuta BERTopic normal con embeddings + clustering.
    Esto asegura que siempre exista al menos un `topic` incluso en documentos m√≠nimos.

* **5. Post-procesamiento y limpieza**:
  Para cada cluster (topic) se selecciona:

  * `exemplar`: bloque de texto m√°s representativo.
  * `keywords`: lista de palabras clave filtradas (removiendo stopwords, duplicados, ruido corto).
  * `count`: n√∫mero de bloques asignados.
    Adem√°s se calculan m√©tricas globales como `outlier_ratio` (proporci√≥n de bloques no asignados a ning√∫n cluster v√°lido).

**Enriquecimiento agregado a `meta.topics_doc`:**

* `n_topics`: n√∫mero total de t√≥picos encontrados o inferidos.
* `keywords_global`: lista unificada de keywords m√°s frecuentes en todo el documento.
* `topics`: listado detallado de cada topic con `id`, `count`, `exemplar`, `keywords`.
* `reason`: explica el modo usado (`doc-level`, `doc-fallback-small`, `doc-fallback-error`).
* `outlier_ratio`: porcentaje de bloques descartados como outliers (cuando aplica).

**Ejemplo de salida:**

```json
"topics_doc": {
  "reason": "doc-level",
  "n_topics": 2,
  "keywords_global": ["diabetes","insulina","s√≠ntomas"],
  "topics": [
    {
      "topic_id": 0,
      "count": 5,
      "exemplar": "S√≠ntomas Cl√≠nicos m√°s frecuentes...",
      "keywords": ["s√≠ntomas","micci√≥n","aumento"]
    },
    {
      "topic_id": 1,
      "count": 4,
      "exemplar": "Diabetes Tipo 2: Un enfoque cl√≠nico",
      "keywords": ["diabetes","tratamiento","complicaciones"]
    }
  ]
}
```

---

#### 3) HybridChunker ‚úÖ

**Entrada:** `DocumentIR+Topics` (`outputs_doc_topics/*.json`)

**Salida:** `DocumentChunks` (`outputs_chunks/*.json`)

**Qu√© hace (paso a paso):**

1. **Extracci√≥n de bloques base**

   * Toma todos los bloques textuales del `DocumentIR`.
   * Cada bloque conserva trazabilidad (`page_number`, `block_indices`) para poder mapear chunks a posiciones exactas en el documento.
   * Filtra bloques vac√≠os o no textuales (im√°genes, tablas sin OCR).

2. **Segmentaci√≥n sem√°ntica h√≠brida**
   Se combinan varias estrategias para dividir el documento en **chunks coherentes de ‚â§2048 tokens** (√≥ptimo para LLMs):

   * **Reglas de headings:** patrones t√≠picos (`Introducci√≥n`, `M√©todos`, `Conclusiones`, etc.) detectados v√≠a regex.
     Esto evita cortar secciones tem√°ticas de forma arbitraria.

   * **spaCy sentence boundaries:** segmenta p√°rrafos largos en oraciones completas, preservando la coherencia sint√°ctica.
     Si spaCy no est√° disponible, se aplica un fallback mediante puntuaci√≥n (`.`, `;`, `?`, `!`) o saltos de l√≠nea dobles (`\n\n`).

   * **Empaquetado sem√°ntico por longitud:** agrupa oraciones hasta alcanzar un l√≠mite aproximado de tokens.
     Controla umbrales de tama√±o (`min_chars`, `max_chars`, `max_tokens`) para mantener chunks **equilibrados** en densidad y contexto.
     ‚Üí El resultado son **unidades estables**: suficientemente largas para el contexto, pero sin sobrepasar los l√≠mites √≥ptimos de procesamiento.

3. **Herencia de contexto (`topic_hints`)**

   * Cada chunk hereda informaci√≥n del doc-level contextizer (`topic_ids`, `keywords_global`, `topic_affinity`).
   * Esto asegura **consistencia sem√°ntica vertical**: lo que el documento conoce a nivel global se transfiere a los fragmentos locales.
   * La afinidad entre el chunk y los t√≥picos globales se calcula mediante una mezcla sem√°ntico-l√©xica:

     $$\text{topic affinity blend}(c, t) = \alpha \cdot \cos(\vec{c}, \vec{t}) + (1 - \alpha) \cdot J(c, t)$$

     Donde:

     * $\vec{c}$ y $\vec{t}$ son los embeddings del chunk y del topic.
     * $J(c, t)$ es la similitud l√©xica (*Jaccard*).
     * $\alpha$ controla el peso sem√°ntico (por defecto, $\alpha = 0.7$).

     Esta combinaci√≥n hace al sistema m√°s robusto frente a textos cortos (tweets, cl√°usulas legales o notas cl√≠nicas) donde la sem√°ntica sola puede ser insuficiente.

4. **C√°lculo de m√©tricas locales**
   
   Para evaluar la calidad y la coherencia de los chunks, se calculan m√©tricas cuantitativas:

   * `cohesion_vs_doc`: mide la similitud coseno entre el embedding del chunk y el embedding promedio del documento.
     Representa **qu√© tan bien el fragmento conserva el contexto global**.

     $$\text{cohesion vs doc}(c_i) = \cos(\vec{c_i}, \bar{\vec{D}})$$

   * `max_redundancy`: mide la similitud m√°xima del chunk con cualquier otro chunk dentro del mismo documento.
     Detecta **fragmentos repetitivos o duplicados**.

     $$\text{max redundancy}(c_i) = \max_{j \neq i} \cos(\vec{c_i}, \vec{c_j})$$

   * `redundancy_norm`: versi√≥n normalizada de la redundancia, que ajusta el valor seg√∫n el tama√±o relativo del fragmento.

     $$\text{redundancy norm}(c_i) = \text{max redundancy}(c_i) \times \frac{\text{len}(c_i)}{\text{avg len(chunks)}}$$

     Esto penaliza m√°s a los chunks **largos y redundantes**, y reduce el impacto de los fragmentos **cortos pero similares**.
     En textos como contratos, rese√±as o tweets, mejora la detecci√≥n de contenido **repetitivo** versus **informativo**.

   * `novelty`: mide la proporci√≥n de informaci√≥n nueva que aporta cada fragmento.
     Es complementaria a la redundancia.

     $$\text{novelty}(c_i) = 1 - \text{max redundancy}(c_i)$$

     Un valor alto de `novelty` indica que el chunk aporta **contexto √∫nico o evidencia nueva**.

   * `chunk_health`: m√©trica compuesta que pondera la cohesi√≥n y la novedad penalizando la redundancia.
     Resume la **salud sem√°ntica del fragmento**.

     $$\text{chunk health}(c_i) = \text{cohesion vs doc}(c_i) \times (1 - \text{max redundancy}(c_i))$$

     Este score puede usarse en etapas posteriores (por ejemplo, el **Adaptive Schema Selector**) para **ponderar o filtrar chunks** seg√∫n su calidad sem√°ntica.

5. **Serializaci√≥n robusta**

   * Cada chunk se guarda con un `chunk_id` √∫nico, metadatos (`doc_id`, idioma, embeddings opcionales) y trazabilidad (`source_spans`).
   * El formato JSON conserva compatibilidad con las etapas siguientes (`contextize-chunks`, `schema-select`).
   * Si los embeddings o spaCy no est√°n disponibles, aplica **fallbacks autom√°ticos** para mantener la robustez del pipeline.

* **Ejemplo de salida (simplificado):**

```json
{
  "chunk_id": "DOC-123_0001",
  "text": "Complicaciones Asociadas al tratamiento prolongado...",
  "topic_hints": {
    "inherited_keywords": ["diabetes", "tratamiento"],
    "inherited_topic_ids": [0, 1],
    "topic_affinity_blend": {"0": 0.82, "1": 0.71}
  },
  "scores": {
    "cohesion_vs_doc": 0.82,
    "max_redundancy": 0.59,
    "redundancy_norm": 0.73,
    "novelty": 0.41,
    "chunk_health": 0.34
  },
  "meta_local": {
    "embedding_model": "all-MiniLM-L6-v2",
    "lang": "es"
  }
}
```


#### 4) BERTopic Contextizer (chunk-level) ‚úÖ

* **Entrada:** `DocumentChunks`

* **Salida:** `Chunks+Topics`

* **Qu√© hace:**

  * Recalcula embeddings para cada chunk.
  * Intenta descubrir **subtemas locales** con BERTopic.
  * **L√≥gica de fallback**:

    * `n_samples = 0` ‚Üí no hay texto, se omite.
    * `n_samples < 5` ‚Üí usa fallback por frecuencia de t√©rminos.
    * `n_samples ‚â• 5` ‚Üí corre BERTopic normal.
  * Cada chunk queda enriquecido con:

    * `topic` (id, keywords, prob).
    * `topic_hints` (heredados de doc-level).

* **Ejemplo de salida (fallback):**

```json
"topics_chunks": {
  "reason": "chunk-fallback-small",
  "n_samples": 3,
  "n_topics": 1,
  "keywords_global": ["diabetes","tratamiento","insulina"],
  "topics": [
    {
      "topic_id": 0,
      "count": 3,
      "exemplar": "Diabetes Tipo 2: Un enfoque cl√≠nico...",
      "keywords": ["diabetes","complicaciones","tratamiento"]
    }
  ]
}
```

---

### 5) Adaptive Schema Selector ‚úÖ

**Entrada:** `Chunks+Topics` (`outputs_chunks/*.json`)
**Salida:** `SchemaSelection` (`outputs_schema/*.json`)

**Qu√© hace:**

El **Adaptive Schema Selector** determina din√°micamente qu√© **dominios de entidades** (ej. m√©dico, legal, financiero, gen√©rico) son relevantes para cada documento y chunk. Esto evita extraer entidades irrelevantes y mejora la **precisi√≥n** del grafo.

1. **Registro de dominios (`registry.py`)**
   Cada dominio contiene:

   * Entidades (`EntityTypeDef`) con atributos (ej. `Disease`, `Treatment`).
   * Relaciones (`RelationTypeDef`) entre entidades.
   * Aliases y vocabulario espec√≠fico (ej. `"enfermedad"`, `"patolog√≠a"` para `Disease`).

2. **Extracci√≥n de se√±ales del documento/chunk**

   * **Keywords**: se buscan overlaps entre tokens y aliases.
   * **Embeddings**: se calcula similitud coseno entre centroides del texto y embeddings predefinidos de etiquetas (`label_vecs`).
   * **Priors**: se aplican pesos de confianza inicial (ej. `generic` siempre se incluye con peso bajo).

3. **F√≥rmula de scoring (por dominio):**

   Para cada dominio (d):

$$
\text{score}(d) = \alpha \cdot S_{\text{kw}}(d) + \beta \cdot S_{\text{emb}}(d) + \gamma \cdot P(d)
$$

Donde:

* $S_{\text{kw}}$: score normalizado por overlap de keywords.
* $S_{\text{emb}}$: similitud coseno entre embeddings.
* $P(d)$: prior asignado al dominio.
* $\alpha, \beta, \gamma$ ‚Üí hiperpar√°metros configurables en `SelectorConfig`.

**Ejemplo default:**

$\alpha = 0.6, \beta = 0.3, \gamma = 0.1$ ‚Üí m√°s peso a keywords, menos a embeddings y priors.

4. **Selecci√≥n final**

   * Se ordenan los dominios por score.
   * Se descartan dominios con score < `min_topic_conf`.
   * Se seleccionan los `top-k` dominios (por config).
   * Se marca `ambiguous=True` si la diferencia entre primer y segundo dominio < `ambiguity_threshold`.
   * Siempre se incluye el dominio **gen√©rico** como fallback (`allow_fallback_generic=True`).

---

**Ejemplo de salida (simplificado):**

```json
{
  "doc": {
    "doc_id": "DOC-123",
    "top_domains": ["medical"],
    "ambiguous": false,
    "domain_scores": [
      {"domain":"medical","score":0.18,"evidence":[{"kind":"keyword","detail":{"overlap":2,"kw_score":0.2}},{"kind":"stat","detail":{"alpha":0.6,"beta":0.3}}]},
      {"domain":"legal","score":0.05,"evidence":[{"kind":"keyword","detail":{"overlap":0,"kw_score":0.0}}]}
    ]
  },
  "chunks": [
    {
      "chunk_id": "DOC-123_0001",
      "top_domain": "medical",
      "ambiguous": false,
      "domain_scores": [...]
    }
  ],
  "meta": {
    "alpha": 0.6,
    "beta": 0.3,
    "gamma": 0.1,
    "always_include": ["generic"],
    "ambiguity_threshold": 0.1
  }
}
```

---

### 6) Mentions (NER/RE) üîú

* Detecta menciones de entidades **condicionadas por t√≥picos**.

### 7‚Äì11) Clustering ‚Üí Label ‚Üí LLM ‚Üí Normalizaci√≥n ‚Üí Grafo üîú

* Agrupan spans, etiquetan con weak supervision, refinan con LLM y normalizan entidades.
* Export final a **grafo de conocimiento**.

---


# üìã Flujo de informaci√≥n en T2G (Contrado de datos)


| Etapa / Subsistema                  | **Entrada (qu√© toma)**                  | **Salida (qu√© agrega / construye)**                                                                               |
| ----------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **1. Parser** (Doc ‚Üí IR)            | Documento bruto (PDF, DOCX, IMG)        | `DocumentIR`: texto por bloques, tablas, metadatos (`mime`, `pages`, `sha256`, `source_path`).                    |
| **2. Contextizer (doc-level)**      | `DocumentIR.pages.blocks.text`          | `meta.topics_doc`: t√≥picos globales, keywords generales, ejemplares, `outlier_ratio`.                             |
| **3. HybridChunker**                | `DocumentIR+Topics`                     | `chunks[*]`: segmentos ‚â§2048 tokens. Heredan `topic_hints` + m√©tricas (`cohesion`, `redundancy`).                 |
| **4. Contextizer (chunk-level)**    | `chunks.text` + `topic_hints` heredados | `chunks[*].topic`: t√≥pico local con `topic_id`, `keywords`, `prob` + `meta.topics_chunks`.                        |
| **5. Schema Selector (adaptativo)** | `chunks+topics` + `registry`            | `SchemaSelection`: dominios relevantes por doc y chunk + `evidence_trace` (keywords, embeddings, priors, scores). |
                            |

---
## üìÇ Pipeline declarativo (YAML)

Archivo: `pipelines/pipeline.yaml`

```yaml
pipeline:
  dry_run: false
  continue_on_error: true

stages:
  - name: parse
    args:
      clean_outdir: true
      inputs_glob:
        - "docs/*.pdf"
        - "docs/*.png"
        - "docs/*.jpg"
        - "docs/*.docx"
      outdir: "outputs_ir"

  - name: contextize-doc
    args:
      clean_outdir: true
      ir_glob: "outputs_ir/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      nr_topics: null
      seed: 42
      outdir: "outputs_doc_topics"

  - name: chunk
    args:
      clean_outdir: true
      ir_glob: "outputs_doc_topics/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      spacy_model: "es_core_news_sm"
      max_tokens: 2048
      min_chars: 280
      outdir: "outputs_chunks"

  - name: contextize-chunks
    args:
      clean_outdir: false
      ir_glob: "outputs_chunks/*.json"
      embedding_model: "all-MiniLM-L6-v2"
      nr_topics: null
      seed: 42
      outdir: "outputs_chunks"
      
  - name: schema-select
    args:
      chunks_glob: "outputs_chunks/*.json"
      outdir: "outputs_schema"
      clean_outdir: true
      alpha_kw: 0.6
      beta_emb: 0.3
      gamma_prior: 0.1
      ambig_margin: 0.08
      topk_domains: 2
      topk_entity_types: 5
```

Ejecutar:

```bash
python t2g_cli.py pipeline-yaml
```

---

## üìä M√©tricas por subsistema

### Parser 

* `percent_docs_ok`: √©xito de parseo por lote.
* `layout_loss`: p√©rdida de estructura.
* `table_consistency`: tablas detectadas vs esperadas.

### Contextizer (doc-level) 

* `coverage`: proporci√≥n de chunks asignados a alg√∫n t√≥pico.
* `outlier_rate`: ratio de outliers vs asignaciones v√°lidas.
* `topic_size_stats`: distribuci√≥n (min, mediana, p95).
* `keywords_diversity`: diversidad de keywords √∫nicas.

### HybridChunker**

* `chunk_length_stats`: distribuci√≥n de tama√±o en caracteres/tokens.
* `cohesion_vs_doc`: similitud coseno chunk ‚Üî doc (coherencia sem√°ntica global).
* `max_redundancy`: similitud m√°xima entre chunks (redundancia bruta).
* `redundancy_norm`: redundancia normalizada seg√∫n longitud media del documento  
  *(penaliza fragmentos largos y repetitivos)*.


### Contextizer (chunk-level)**

  * `coverage`: % de chunks con topic asignado.
  * `fallback_rate`: % de documentos donde se us√≥ fallback vs BERTopic.
  * `topic_size_stats`: tama√±o medio de clusters locales.

### Adaptive Schema Selector**

  * `domain_score_distribution`: histograma de scores por dominio.
  * `ambiguity_rate`: % de documentos/chunks con `ambiguous=True`.
  * `coverage_domains`: promedio de dominios relevantes por documento.
  * `evidence_trace`: lista de evidencias usadas para cada score (auditor√≠a).

---

## ‚öôÔ∏è Instalaci√≥n

### Requisitos principales

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### OCR

```bash
brew install tesseract tesseract-lang   # macOS
sudo apt install tesseract-ocr-spa      # Ubuntu/Debian
```

### NLP / embeddings

```bash
pip install spacy sentence-transformers bertopic
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
```

### Opcionales

```bash
pip install torch joblib matplotlib wordcloud
```

---

## üöÄ Uso r√°pido (CLI)

```bash
# Parser ‚Üí IR
python t2g_cli.py parse docs/ejemplo.pdf --outdir outputs_ir

# Contextizer (doc) ‚Üí IR+Topics
python t2g_cli.py contextize-doc outputs_ir/*.json --outdir outputs_doc_topics

# Pipeline completo
python t2g_cli.py pipeline-yaml
```

---

## üõ†Ô∏è Troubleshooting

* **JSONs vac√≠os:** borra outputs y reejecuta.
* **OCR falla:** instala Tesseract y revisa `PATH`.
* **spaCy no carga:** descarga modelos `es_core_news_sm`, `en_core_web_sm`.
* **Errores de import:** ejecuta siempre desde ra√≠z del proyecto.

---

## üß™ Roadmap inmediato

* A√±adir **HybridChunker** con herencia de `topics_doc`.
* Implementar **chunk-level contextizer**.
* Integrar **Adaptive Schema Selector**.
* Completar **Mentions + Clustering + Normalizaci√≥n**.
* Exportar entidades y relaciones a **Neo4j / RDF**.

